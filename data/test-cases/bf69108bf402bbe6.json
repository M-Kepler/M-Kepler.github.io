{"uid":"bf69108bf402bbe6","name":"test_multi_chat[flow_response-KimiHelper]","fullName":"lib.llm.test_llm.TestLLM#test_multi_chat","historyId":"52ee1572ba05f7d84a04de48b30b77df","time":{"start":1715238458017,"stop":1715238561572,"duration":103555},"description":"\n        多轮对话\n        ","descriptionHtml":"<pre><code>    多轮对话\n</code></pre>\n","status":"broken","statusMessage":"tenacity.RetryError: RetryError[<Future at 0x7f5796b25b50 state=finished raised RateLimitError>]","statusTrace":"self = <Retrying object at 0x7f57ae0f89a0 (stop=<tenacity.stop.stop_after_attempt object at 0x7f57ae0f87f0>, wait=<tenacity.w...0x7f57ae0f8730>, before=<function before_nothing at 0x7f57ae0d7ca0>, after=<function after_nothing at 0x7f57ae0e1040>)>\nfn = <function OpenAIBaseHelper.chat at 0x7f57ae0fa4c0>\nargs = (<mypy.lib.llm.kimi.helper.KimiHelper object at 0x7f57981e6220>, True)\nkwargs = {}\nretry_state = <RetryCallState 140014177670480: attempt #5; slept for 36.0; last result: failed (RateLimitError Error code: 429 - {'e...g5g111fix4h1> request reached max request: 3, please try again after 1 seconds', 'type': 'rate_limit_reached_error'}})>\ndo = <tenacity.DoAttempt object at 0x7f5796b25460>\n\n    def __call__(\n        self,\n        fn: t.Callable[..., WrappedFnReturnT],\n        *args: t.Any,\n        **kwargs: t.Any,\n    ) -> WrappedFnReturnT:\n        self.begin()\n    \n        retry_state = RetryCallState(retry_object=self, fn=fn, args=args, kwargs=kwargs)\n        while True:\n            do = self.iter(retry_state=retry_state)\n            if isinstance(do, DoAttempt):\n                try:\n>                   result = fn(*args, **kwargs)\n\n/opt/hostedtoolcache/Python/3.9.19/x64/lib/python3.9/site-packages/tenacity/__init__.py:470: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nmypy/lib/llm/core/helper.py:103: in chat\n    response = self._client.chat.completions.create(model=self._model,\n/opt/hostedtoolcache/Python/3.9.19/x64/lib/python3.9/site-packages/openai/_utils/_utils.py:277: in wrapper\n    return func(*args, **kwargs)\n/opt/hostedtoolcache/Python/3.9.19/x64/lib/python3.9/site-packages/openai/resources/chat/completions.py:590: in create\n    return self._post(\n/opt/hostedtoolcache/Python/3.9.19/x64/lib/python3.9/site-packages/openai/_base_client.py:1240: in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n/opt/hostedtoolcache/Python/3.9.19/x64/lib/python3.9/site-packages/openai/_base_client.py:921: in request\n    return self._request(\n/opt/hostedtoolcache/Python/3.9.19/x64/lib/python3.9/site-packages/openai/_base_client.py:1005: in _request\n    return self._retry_request(\n/opt/hostedtoolcache/Python/3.9.19/x64/lib/python3.9/site-packages/openai/_base_client.py:1053: in _retry_request\n    return self._request(\n/opt/hostedtoolcache/Python/3.9.19/x64/lib/python3.9/site-packages/openai/_base_client.py:1005: in _request\n    return self._retry_request(\n/opt/hostedtoolcache/Python/3.9.19/x64/lib/python3.9/site-packages/openai/_base_client.py:1053: in _retry_request\n    return self._request(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x7f57981e6f70>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        remaining_retries: int | None,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        self._prepare_options(options)\n    \n        retries = self._remaining_retries(remaining_retries, options)\n        request = self._build_request(options)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n            response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n        except httpx.TimeoutException as err:\n            log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n            if retries > 0:\n                return self._retry_request(\n                    options,\n                    cast_to,\n                    retries,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising timeout error\")\n            raise APITimeoutError(request=request) from err\n        except Exception as err:\n            log.debug(\"Encountered Exception\", exc_info=True)\n    \n            if retries > 0:\n                return self._retry_request(\n                    options,\n                    cast_to,\n                    retries,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising connection error\")\n            raise APIConnectionError(request=request) from err\n    \n        log.debug(\n            'HTTP Response: %s %s \"%i %s\" %s',\n            request.method,\n            request.url,\n            response.status_code,\n            response.reason_phrase,\n            response.headers,\n        )\n        log.debug(\"request_id: %s\", response.headers.get(\"x-request-id\"))\n    \n        try:\n            response.raise_for_status()\n        except httpx.HTTPStatusError as err:  # thrown on 4xx and 5xx status code\n            log.debug(\"Encountered httpx.HTTPStatusError\", exc_info=True)\n    \n            if retries > 0 and self._should_retry(err.response):\n                err.response.close()\n                return self._retry_request(\n                    options,\n                    cast_to,\n                    retries,\n                    err.response.headers,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                )\n    \n            # If the response is streamed then we need to explicitly read the response\n            # to completion before attempting to access the response text.\n            if not err.response.is_closed:\n                err.response.read()\n    \n            log.debug(\"Re-raising status error\")\n>           raise self._make_status_error_from_response(err.response) from None\nE           openai.RateLimitError: Error code: 429 - {'error': {'message': 'Your account co2mu8o3r072r8fgnkv0<ak-erfxkb8jg5g111fix4h1> request reached max request: 3, please try again after 1 seconds', 'type': 'rate_limit_reached_error'}}\n\n/opt/hostedtoolcache/Python/3.9.19/x64/lib/python3.9/site-packages/openai/_base_client.py:1020: RateLimitError\n\nThe above exception was the direct cause of the following exception:\n\nself = <test_llm.TestLLM object at 0x7f579853b4c0>, use_stream_response = True\nllm_provider = <class 'mypy.lib.llm.kimi.helper.KimiHelper'>\n\n    @pytest.mark.parametrize(\n        'use_stream_response', [True, False],\n        ids=['flow_response', 'direct_response'])\n    def test_multi_chat(self, use_stream_response, llm_provider):\n        \"\"\"\n        多轮对话\n        \"\"\"\n        system_set = '你是一个数学家'\n        user_asks = ['1888 乘以 3212等于多少', '继续减去 1314 后是多少']\n        llm_api: OpenAIBaseHelper = llm_provider()\n    \n        # step1. 预设角色\n        llm_api.ask_helper.add_ask_message(\n            message=system_set,\n            role=llm_api.ask_helper.ROLE_SYSTEM)\n        # step2. 开始问\n        for ask_msg in user_asks:\n            llm_api.ask_helper.add_ask_message(message=ask_msg)\n    \n        # step3. 向 AI 提问\n>       result = llm_api.chat(use_stream=use_stream_response)\n\nmypy/tests/lib/llm/test_llm.py:68: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nmypy/lib/llm/kimi/helper.py:18: in chat\n    return super().chat(use_stream)\n/opt/hostedtoolcache/Python/3.9.19/x64/lib/python3.9/site-packages/tenacity/__init__.py:330: in wrapped_f\n    return self(f, *args, **kw)\n/opt/hostedtoolcache/Python/3.9.19/x64/lib/python3.9/site-packages/tenacity/__init__.py:467: in __call__\n    do = self.iter(retry_state=retry_state)\n/opt/hostedtoolcache/Python/3.9.19/x64/lib/python3.9/site-packages/tenacity/__init__.py:368: in iter\n    result = action(retry_state)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nrs = <RetryCallState 140014177670480: attempt #5; slept for 36.0; last result: failed (RateLimitError Error code: 429 - {'e...g5g111fix4h1> request reached max request: 3, please try again after 1 seconds', 'type': 'rate_limit_reached_error'}})>\n\n    def exc_check(rs: \"RetryCallState\") -> None:\n        fut = t.cast(Future, rs.outcome)\n        retry_exc = self.retry_error_cls(fut)\n        if self.reraise:\n            raise retry_exc.reraise()\n>       raise retry_exc from fut.exception()\nE       tenacity.RetryError: RetryError[<Future at 0x7f5796b25b50 state=finished raised RateLimitError>]\n\n/opt/hostedtoolcache/Python/3.9.19/x64/lib/python3.9/site-packages/tenacity/__init__.py:411: RetryError","flaky":false,"newFailed":false,"newBroken":false,"newPassed":false,"retriesCount":0,"retriesStatusChange":false,"beforeStages":[{"name":"event_loop_policy","time":{"start":1715236488701,"stop":1715236488702,"duration":1},"status":"passed","steps":[],"attachments":[],"parameters":[],"stepsCount":0,"attachmentsCount":0,"hasContent":false,"attachmentStep":false,"shouldDisplayMessage":false},{"name":"copy_config_to_dir","time":{"start":1715238163260,"stop":1715238163261,"duration":1},"status":"passed","steps":[],"attachments":[],"parameters":[],"stepsCount":0,"attachmentsCount":0,"hasContent":false,"attachmentStep":false,"shouldDisplayMessage":false}],"afterStages":[{"name":"copy_config_to_dir::finish_callback","time":{"start":1715238933059,"stop":1715238933059,"duration":0},"status":"passed","steps":[],"attachments":[],"parameters":[],"stepsCount":0,"attachmentsCount":0,"hasContent":false,"attachmentStep":false,"shouldDisplayMessage":false}],"labels":[{"name":"parentSuite","value":"lib.llm"},{"name":"suite","value":"test_llm"},{"name":"subSuite","value":"TestLLM"},{"name":"host","value":"fv-az1145-772"},{"name":"thread","value":"6357-MainThread"},{"name":"framework","value":"pytest"},{"name":"language","value":"cpython3"},{"name":"package","value":"lib.llm.test_llm"},{"name":"resultFormat","value":"allure2"}],"parameters":[{"name":"llm_provider","value":"<class 'mypy.lib.llm.kimi.helper.KimiHelper'>"},{"name":"use_stream_response","value":"True"}],"links":[],"hidden":false,"retry":false,"extra":{"severity":"normal","retries":[],"categories":[{"name":"Test defects","matchedStatuses":[],"flaky":false}],"tags":[]},"source":"bf69108bf402bbe6.json","parameterValues":["<class 'mypy.lib.llm.kimi.helper.KimiHelper'>","True"]}