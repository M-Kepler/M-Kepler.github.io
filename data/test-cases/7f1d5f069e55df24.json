{"uid":"7f1d5f069e55df24","name":"test_xiaohongshu[KimiHelper]","fullName":"lib.llm.test_llm.TestLLM#test_xiaohongshu","historyId":"3ce0eb0dfc8365a5ea3950cfbcd16867","time":{"start":1719585564260,"stop":1719585651812,"duration":87552},"status":"broken","statusMessage":"tenacity.RetryError: RetryError[<Future at 0x7fc3d8c06c40 state=finished raised RateLimitError>]","statusTrace":"self = <Retrying object at 0x7fc3f76e0be0 (stop=<tenacity.stop.stop_after_attempt object at 0x7fc3fcc95e80>, wait=<tenacity.w...0x7fc3fcc95e20>, before=<function before_nothing at 0x7fc3fcce6b80>, after=<function after_nothing at 0x7fc3fcce6ee0>)>\nfn = <function OpenAIBaseHelper.chat at 0x7fc3fcc8fdc0>\nargs = (<mypy.lib.llm.kimi.helper.KimiHelper object at 0x7fc3d8e11160>, False)\nkwargs = {}\nretry_state = <RetryCallState 140479646534720: attempt #5; slept for 36.0; last result: failed (RateLimitError Error code: 429 - {'e...g5g111fix4h1> request reached max request: 3, please try again after 1 seconds', 'type': 'rate_limit_reached_error'}})>\ndo = <tenacity.DoAttempt object at 0x7fc3d8c192b0>\n\n    def __call__(\n        self,\n        fn: t.Callable[..., WrappedFnReturnT],\n        *args: t.Any,\n        **kwargs: t.Any,\n    ) -> WrappedFnReturnT:\n        self.begin()\n    \n        retry_state = RetryCallState(retry_object=self, fn=fn, args=args, kwargs=kwargs)\n        while True:\n            do = self.iter(retry_state=retry_state)\n            if isinstance(do, DoAttempt):\n                try:\n>                   result = fn(*args, **kwargs)\n\n/opt/hostedtoolcache/Python/3.9.19/x64/lib/python3.9/site-packages/tenacity/__init__.py:478: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nmypy/lib/llm/core/helper.py:103: in chat\n    response = self._client.chat.completions.create(model=self._model,\n/opt/hostedtoolcache/Python/3.9.19/x64/lib/python3.9/site-packages/openai/_utils/_utils.py:277: in wrapper\n    return func(*args, **kwargs)\n/opt/hostedtoolcache/Python/3.9.19/x64/lib/python3.9/site-packages/openai/resources/chat/completions.py:643: in create\n    return self._post(\n/opt/hostedtoolcache/Python/3.9.19/x64/lib/python3.9/site-packages/openai/_base_client.py:1250: in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n/opt/hostedtoolcache/Python/3.9.19/x64/lib/python3.9/site-packages/openai/_base_client.py:931: in request\n    return self._request(\n/opt/hostedtoolcache/Python/3.9.19/x64/lib/python3.9/site-packages/openai/_base_client.py:1015: in _request\n    return self._retry_request(\n/opt/hostedtoolcache/Python/3.9.19/x64/lib/python3.9/site-packages/openai/_base_client.py:1063: in _retry_request\n    return self._request(\n/opt/hostedtoolcache/Python/3.9.19/x64/lib/python3.9/site-packages/openai/_base_client.py:1015: in _request\n    return self._retry_request(\n/opt/hostedtoolcache/Python/3.9.19/x64/lib/python3.9/site-packages/openai/_base_client.py:1063: in _retry_request\n    return self._request(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x7fc3d8e11a00>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        remaining_retries: int | None,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        self._prepare_options(options)\n    \n        retries = self._remaining_retries(remaining_retries, options)\n        request = self._build_request(options)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n            response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n        except httpx.TimeoutException as err:\n            log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n            if retries > 0:\n                return self._retry_request(\n                    options,\n                    cast_to,\n                    retries,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising timeout error\")\n            raise APITimeoutError(request=request) from err\n        except Exception as err:\n            log.debug(\"Encountered Exception\", exc_info=True)\n    \n            if retries > 0:\n                return self._retry_request(\n                    options,\n                    cast_to,\n                    retries,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising connection error\")\n            raise APIConnectionError(request=request) from err\n    \n        log.debug(\n            'HTTP Response: %s %s \"%i %s\" %s',\n            request.method,\n            request.url,\n            response.status_code,\n            response.reason_phrase,\n            response.headers,\n        )\n        log.debug(\"request_id: %s\", response.headers.get(\"x-request-id\"))\n    \n        try:\n            response.raise_for_status()\n        except httpx.HTTPStatusError as err:  # thrown on 4xx and 5xx status code\n            log.debug(\"Encountered httpx.HTTPStatusError\", exc_info=True)\n    \n            if retries > 0 and self._should_retry(err.response):\n                err.response.close()\n                return self._retry_request(\n                    options,\n                    cast_to,\n                    retries,\n                    err.response.headers,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                )\n    \n            # If the response is streamed then we need to explicitly read the response\n            # to completion before attempting to access the response text.\n            if not err.response.is_closed:\n                err.response.read()\n    \n            log.debug(\"Re-raising status error\")\n>           raise self._make_status_error_from_response(err.response) from None\nE           openai.RateLimitError: Error code: 429 - {'error': {'message': 'Your account co2mu8o3r072r8fgnkv0<ak-erfxkb8jg5g111fix4h1> request reached max request: 3, please try again after 1 seconds', 'type': 'rate_limit_reached_error'}}\n\n/opt/hostedtoolcache/Python/3.9.19/x64/lib/python3.9/site-packages/openai/_base_client.py:1030: RateLimitError\n\nThe above exception was the direct cause of the following exception:\n\nself = <test_llm.TestLLM object at 0x7fc3e11f1640>\nllm_provider = <class 'mypy.lib.llm.kimi.helper.KimiHelper'>\n\n    def test_xiaohongshu(self, llm_provider):\n        print('----- %s -----', llm_provider.__name__)\n        subject = '珠海旅游攻略'\n        llm_api: OpenAIBaseHelper = llm_provider()\n        llm_api.ask_helper.add_ask_message(\n            message=Xiaohongshu.title(subject))\n        title_result = llm_api.chat()\n        log_line(LOG)\n        LOG.info('小红书标题:')\n        LOG.info(title_result)\n        log_line(LOG)\n    \n        llm_api.ask_helper.add_ask_message(\n            message=Xiaohongshu.post(subject))\n>       post_result = llm_api.chat()\n\nmypy/tests/lib/llm/test_llm.py:95: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nmypy/lib/llm/kimi/helper.py:18: in chat\n    return super().chat(use_stream)\n/opt/hostedtoolcache/Python/3.9.19/x64/lib/python3.9/site-packages/tenacity/__init__.py:336: in wrapped_f\n    return copy(f, *args, **kw)\n/opt/hostedtoolcache/Python/3.9.19/x64/lib/python3.9/site-packages/tenacity/__init__.py:475: in __call__\n    do = self.iter(retry_state=retry_state)\n/opt/hostedtoolcache/Python/3.9.19/x64/lib/python3.9/site-packages/tenacity/__init__.py:376: in iter\n    result = action(retry_state)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nrs = <RetryCallState 140479646534720: attempt #5; slept for 36.0; last result: failed (RateLimitError Error code: 429 - {'e...g5g111fix4h1> request reached max request: 3, please try again after 1 seconds', 'type': 'rate_limit_reached_error'}})>\n\n    def exc_check(rs: \"RetryCallState\") -> None:\n        fut = t.cast(Future, rs.outcome)\n        retry_exc = self.retry_error_cls(fut)\n        if self.reraise:\n            raise retry_exc.reraise()\n>       raise retry_exc from fut.exception()\nE       tenacity.RetryError: RetryError[<Future at 0x7fc3d8c06c40 state=finished raised RateLimitError>]\n\n/opt/hostedtoolcache/Python/3.9.19/x64/lib/python3.9/site-packages/tenacity/__init__.py:419: RetryError","flaky":false,"newFailed":false,"newBroken":false,"newPassed":false,"retriesCount":0,"retriesStatusChange":false,"beforeStages":[{"name":"event_loop_policy","time":{"start":1719581870889,"stop":1719581870890,"duration":1},"status":"passed","steps":[],"attachments":[],"parameters":[],"shouldDisplayMessage":false,"stepsCount":0,"attachmentsCount":0,"hasContent":false,"attachmentStep":false},{"name":"copy_config_to_dir","time":{"start":1719585450547,"stop":1719585450548,"duration":1},"status":"passed","steps":[],"attachments":[],"parameters":[],"shouldDisplayMessage":false,"stepsCount":0,"attachmentsCount":0,"hasContent":false,"attachmentStep":false}],"testStage":{"status":"broken","statusMessage":"tenacity.RetryError: RetryError[<Future at 0x7fc3d8c06c40 state=finished raised RateLimitError>]","statusTrace":"self = <Retrying object at 0x7fc3f76e0be0 (stop=<tenacity.stop.stop_after_attempt object at 0x7fc3fcc95e80>, wait=<tenacity.w...0x7fc3fcc95e20>, before=<function before_nothing at 0x7fc3fcce6b80>, after=<function after_nothing at 0x7fc3fcce6ee0>)>\nfn = <function OpenAIBaseHelper.chat at 0x7fc3fcc8fdc0>\nargs = (<mypy.lib.llm.kimi.helper.KimiHelper object at 0x7fc3d8e11160>, False)\nkwargs = {}\nretry_state = <RetryCallState 140479646534720: attempt #5; slept for 36.0; last result: failed (RateLimitError Error code: 429 - {'e...g5g111fix4h1> request reached max request: 3, please try again after 1 seconds', 'type': 'rate_limit_reached_error'}})>\ndo = <tenacity.DoAttempt object at 0x7fc3d8c192b0>\n\n    def __call__(\n        self,\n        fn: t.Callable[..., WrappedFnReturnT],\n        *args: t.Any,\n        **kwargs: t.Any,\n    ) -> WrappedFnReturnT:\n        self.begin()\n    \n        retry_state = RetryCallState(retry_object=self, fn=fn, args=args, kwargs=kwargs)\n        while True:\n            do = self.iter(retry_state=retry_state)\n            if isinstance(do, DoAttempt):\n                try:\n>                   result = fn(*args, **kwargs)\n\n/opt/hostedtoolcache/Python/3.9.19/x64/lib/python3.9/site-packages/tenacity/__init__.py:478: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nmypy/lib/llm/core/helper.py:103: in chat\n    response = self._client.chat.completions.create(model=self._model,\n/opt/hostedtoolcache/Python/3.9.19/x64/lib/python3.9/site-packages/openai/_utils/_utils.py:277: in wrapper\n    return func(*args, **kwargs)\n/opt/hostedtoolcache/Python/3.9.19/x64/lib/python3.9/site-packages/openai/resources/chat/completions.py:643: in create\n    return self._post(\n/opt/hostedtoolcache/Python/3.9.19/x64/lib/python3.9/site-packages/openai/_base_client.py:1250: in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n/opt/hostedtoolcache/Python/3.9.19/x64/lib/python3.9/site-packages/openai/_base_client.py:931: in request\n    return self._request(\n/opt/hostedtoolcache/Python/3.9.19/x64/lib/python3.9/site-packages/openai/_base_client.py:1015: in _request\n    return self._retry_request(\n/opt/hostedtoolcache/Python/3.9.19/x64/lib/python3.9/site-packages/openai/_base_client.py:1063: in _retry_request\n    return self._request(\n/opt/hostedtoolcache/Python/3.9.19/x64/lib/python3.9/site-packages/openai/_base_client.py:1015: in _request\n    return self._retry_request(\n/opt/hostedtoolcache/Python/3.9.19/x64/lib/python3.9/site-packages/openai/_base_client.py:1063: in _retry_request\n    return self._request(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x7fc3d8e11a00>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        remaining_retries: int | None,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        self._prepare_options(options)\n    \n        retries = self._remaining_retries(remaining_retries, options)\n        request = self._build_request(options)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n            response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n        except httpx.TimeoutException as err:\n            log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n            if retries > 0:\n                return self._retry_request(\n                    options,\n                    cast_to,\n                    retries,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising timeout error\")\n            raise APITimeoutError(request=request) from err\n        except Exception as err:\n            log.debug(\"Encountered Exception\", exc_info=True)\n    \n            if retries > 0:\n                return self._retry_request(\n                    options,\n                    cast_to,\n                    retries,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising connection error\")\n            raise APIConnectionError(request=request) from err\n    \n        log.debug(\n            'HTTP Response: %s %s \"%i %s\" %s',\n            request.method,\n            request.url,\n            response.status_code,\n            response.reason_phrase,\n            response.headers,\n        )\n        log.debug(\"request_id: %s\", response.headers.get(\"x-request-id\"))\n    \n        try:\n            response.raise_for_status()\n        except httpx.HTTPStatusError as err:  # thrown on 4xx and 5xx status code\n            log.debug(\"Encountered httpx.HTTPStatusError\", exc_info=True)\n    \n            if retries > 0 and self._should_retry(err.response):\n                err.response.close()\n                return self._retry_request(\n                    options,\n                    cast_to,\n                    retries,\n                    err.response.headers,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                )\n    \n            # If the response is streamed then we need to explicitly read the response\n            # to completion before attempting to access the response text.\n            if not err.response.is_closed:\n                err.response.read()\n    \n            log.debug(\"Re-raising status error\")\n>           raise self._make_status_error_from_response(err.response) from None\nE           openai.RateLimitError: Error code: 429 - {'error': {'message': 'Your account co2mu8o3r072r8fgnkv0<ak-erfxkb8jg5g111fix4h1> request reached max request: 3, please try again after 1 seconds', 'type': 'rate_limit_reached_error'}}\n\n/opt/hostedtoolcache/Python/3.9.19/x64/lib/python3.9/site-packages/openai/_base_client.py:1030: RateLimitError\n\nThe above exception was the direct cause of the following exception:\n\nself = <test_llm.TestLLM object at 0x7fc3e11f1640>\nllm_provider = <class 'mypy.lib.llm.kimi.helper.KimiHelper'>\n\n    def test_xiaohongshu(self, llm_provider):\n        print('----- %s -----', llm_provider.__name__)\n        subject = '珠海旅游攻略'\n        llm_api: OpenAIBaseHelper = llm_provider()\n        llm_api.ask_helper.add_ask_message(\n            message=Xiaohongshu.title(subject))\n        title_result = llm_api.chat()\n        log_line(LOG)\n        LOG.info('小红书标题:')\n        LOG.info(title_result)\n        log_line(LOG)\n    \n        llm_api.ask_helper.add_ask_message(\n            message=Xiaohongshu.post(subject))\n>       post_result = llm_api.chat()\n\nmypy/tests/lib/llm/test_llm.py:95: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nmypy/lib/llm/kimi/helper.py:18: in chat\n    return super().chat(use_stream)\n/opt/hostedtoolcache/Python/3.9.19/x64/lib/python3.9/site-packages/tenacity/__init__.py:336: in wrapped_f\n    return copy(f, *args, **kw)\n/opt/hostedtoolcache/Python/3.9.19/x64/lib/python3.9/site-packages/tenacity/__init__.py:475: in __call__\n    do = self.iter(retry_state=retry_state)\n/opt/hostedtoolcache/Python/3.9.19/x64/lib/python3.9/site-packages/tenacity/__init__.py:376: in iter\n    result = action(retry_state)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nrs = <RetryCallState 140479646534720: attempt #5; slept for 36.0; last result: failed (RateLimitError Error code: 429 - {'e...g5g111fix4h1> request reached max request: 3, please try again after 1 seconds', 'type': 'rate_limit_reached_error'}})>\n\n    def exc_check(rs: \"RetryCallState\") -> None:\n        fut = t.cast(Future, rs.outcome)\n        retry_exc = self.retry_error_cls(fut)\n        if self.reraise:\n            raise retry_exc.reraise()\n>       raise retry_exc from fut.exception()\nE       tenacity.RetryError: RetryError[<Future at 0x7fc3d8c06c40 state=finished raised RateLimitError>]\n\n/opt/hostedtoolcache/Python/3.9.19/x64/lib/python3.9/site-packages/tenacity/__init__.py:419: RetryError","steps":[],"attachments":[{"uid":"514d7c4c06eca733","name":"log","source":"514d7c4c06eca733.txt","type":"text/plain","size":891}],"parameters":[],"shouldDisplayMessage":true,"stepsCount":0,"attachmentsCount":1,"hasContent":true,"attachmentStep":false},"afterStages":[{"name":"copy_config_to_dir::finish_callback","time":{"start":1719586128643,"stop":1719586128644,"duration":1},"status":"passed","steps":[],"attachments":[],"parameters":[],"shouldDisplayMessage":false,"stepsCount":0,"attachmentsCount":0,"hasContent":false,"attachmentStep":false}],"labels":[{"name":"parentSuite","value":"lib.llm"},{"name":"suite","value":"test_llm"},{"name":"subSuite","value":"TestLLM"},{"name":"host","value":"fv-az1788-68"},{"name":"thread","value":"7120-MainThread"},{"name":"framework","value":"pytest"},{"name":"language","value":"cpython3"},{"name":"package","value":"lib.llm.test_llm"},{"name":"resultFormat","value":"allure2"}],"parameters":[{"name":"llm_provider","value":"<class 'mypy.lib.llm.kimi.helper.KimiHelper'>"}],"links":[],"hidden":false,"retry":false,"extra":{"severity":"normal","retries":[],"categories":[{"name":"Test defects","matchedStatuses":[],"flaky":false}],"tags":[]},"source":"7f1d5f069e55df24.json","parameterValues":["<class 'mypy.lib.llm.kimi.helper.KimiHelper'>"]}