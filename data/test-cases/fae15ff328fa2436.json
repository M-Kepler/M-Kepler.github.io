{"uid":"fae15ff328fa2436","name":"test_kimi_multi_chat[\\u6d41\\u5f0f\\u54cd\\u5e94]","fullName":"lib.llm.kimi.test_kimi#test_kimi_multi_chat","historyId":"d65928d92c42af3562650ff70a059862","time":{"start":1713494058945,"stop":1713494158900,"duration":99955},"description":"\n    多轮对话\n    ","descriptionHtml":"<pre><code>多轮对话\n</code></pre>\n","status":"broken","statusMessage":"tenacity.RetryError: RetryError[<Future at 0x7fb5931c1af0 state=finished raised RateLimitError>]","statusTrace":"self = <Retrying object at 0x7fb58363a6a0 (stop=<tenacity.stop.stop_after_attempt object at 0x7fb583631550>, wait=<tenacity.w...0x7fb583622b50>, before=<function before_nothing at 0x7fb5855b2ee0>, after=<function after_nothing at 0x7fb5855b91f0>)>\nfn = <function OpenAIBaseHelper.chat at 0x7fb583636310>\nargs = (<mypy.lib.llm.kimi.helper.KimiHelper object at 0x7fb57dbb0520>,)\nkwargs = {'use_stream': True}\nretry_state = <RetryCallState 140417835205680: attempt #5; slept for 36.0; last result: failed (RateLimitError Error code: 429 - {'e...g5g111fix4h1> request reached max requset: 3, please try again after 1 seconds', 'type': 'rate_limit_reached_error'}})>\ndo = <tenacity.DoAttempt object at 0x7fb5931c10d0>\n\n    def __call__(\n        self,\n        fn: t.Callable[..., WrappedFnReturnT],\n        *args: t.Any,\n        **kwargs: t.Any,\n    ) -> WrappedFnReturnT:\n        self.begin()\n    \n        retry_state = RetryCallState(retry_object=self, fn=fn, args=args, kwargs=kwargs)\n        while True:\n            do = self.iter(retry_state=retry_state)\n            if isinstance(do, DoAttempt):\n                try:\n>                   result = fn(*args, **kwargs)\n\n/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/tenacity/__init__.py:382: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nmypy/lib/llm/core/helper.py:83: in chat\n    response = self._client.chat.completions.create(\n/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/openai/_utils/_utils.py:277: in wrapper\n    return func(*args, **kwargs)\n/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/openai/resources/chat/completions.py:581: in create\n    return self._post(\n/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/openai/_base_client.py:1232: in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/openai/_base_client.py:921: in request\n    return self._request(\n/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/openai/_base_client.py:997: in _request\n    return self._retry_request(\n/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/openai/_base_client.py:1045: in _retry_request\n    return self._request(\n/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/openai/_base_client.py:997: in _request\n    return self._retry_request(\n/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/openai/_base_client.py:1045: in _retry_request\n    return self._request(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x7fb593251e50>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        remaining_retries: int | None,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        self._prepare_options(options)\n    \n        retries = self._remaining_retries(remaining_retries, options)\n        request = self._build_request(options)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        try:\n            response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n        except httpx.TimeoutException as err:\n            log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n            if retries > 0:\n                return self._retry_request(\n                    options,\n                    cast_to,\n                    retries,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising timeout error\")\n            raise APITimeoutError(request=request) from err\n        except Exception as err:\n            log.debug(\"Encountered Exception\", exc_info=True)\n    \n            if retries > 0:\n                return self._retry_request(\n                    options,\n                    cast_to,\n                    retries,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising connection error\")\n            raise APIConnectionError(request=request) from err\n    \n        log.debug(\n            'HTTP Request: %s %s \"%i %s\"', request.method, request.url, response.status_code, response.reason_phrase\n        )\n    \n        try:\n            response.raise_for_status()\n        except httpx.HTTPStatusError as err:  # thrown on 4xx and 5xx status code\n            log.debug(\"Encountered httpx.HTTPStatusError\", exc_info=True)\n    \n            if retries > 0 and self._should_retry(err.response):\n                err.response.close()\n                return self._retry_request(\n                    options,\n                    cast_to,\n                    retries,\n                    err.response.headers,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                )\n    \n            # If the response is streamed then we need to explicitly read the response\n            # to completion before attempting to access the response text.\n            if not err.response.is_closed:\n                err.response.read()\n    \n            log.debug(\"Re-raising status error\")\n>           raise self._make_status_error_from_response(err.response) from None\nE           openai.RateLimitError: Error code: 429 - {'error': {'message': 'Your account co2mu8o3r072r8fgnkv0<ak-erfxkb8jg5g111fix4h1> request reached max requset: 3, please try again after 1 seconds', 'type': 'rate_limit_reached_error'}}\n\n/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/openai/_base_client.py:1012: RateLimitError\n\nThe above exception was the direct cause of the following exception:\n\nuse_stream_response = True\n\n    @pytest.mark.parametrize(\n        'use_stream_response', [True, False],\n        ids=['流式响应', '直接响应'])\n    def test_kimi_multi_chat(use_stream_response):\n        \"\"\"\n        多轮对话\n        \"\"\"\n        api = KimiHelper()\n    \n        system_set = '你是一个数学家'\n        user_asks = ['1888 乘以 3212等于多少', '继续减去 1314 后是多少']\n        # step1. 预设角色\n        api.ask_helper.add_ask_message(\n            message=system_set,\n            role=api.ask_helper.ROLE_SYSTEM)\n        # step2. 开始问\n        for ask_msg in user_asks:\n            api.ask_helper.add_ask_message(\n                message=ask_msg,\n                role=api.ask_helper.ROLE_USER)\n    \n            # step3. 像 AI 提问\n>           api.chat(use_stream=use_stream_response)\n\nmypy/tests/lib/llm/kimi/test_kimi.py:43: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/tenacity/__init__.py:289: in wrapped_f\n    return self(f, *args, **kw)\n/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/tenacity/__init__.py:379: in __call__\n    do = self.iter(retry_state=retry_state)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <Retrying object at 0x7fb58363a6a0 (stop=<tenacity.stop.stop_after_attempt object at 0x7fb583631550>, wait=<tenacity.w...0x7fb583622b50>, before=<function before_nothing at 0x7fb5855b2ee0>, after=<function after_nothing at 0x7fb5855b91f0>)>\nretry_state = <RetryCallState 140417835205680: attempt #5; slept for 36.0; last result: failed (RateLimitError Error code: 429 - {'e...g5g111fix4h1> request reached max requset: 3, please try again after 1 seconds', 'type': 'rate_limit_reached_error'}})>\n\n    def iter(self, retry_state: \"RetryCallState\") -> t.Union[DoAttempt, DoSleep, t.Any]:  # noqa\n        fut = retry_state.outcome\n        if fut is None:\n            if self.before is not None:\n                self.before(retry_state)\n            return DoAttempt()\n    \n        is_explicit_retry = fut.failed and isinstance(fut.exception(), TryAgain)\n        if not (is_explicit_retry or self.retry(retry_state)):\n            return fut.result()\n    \n        if self.after is not None:\n            self.after(retry_state)\n    \n        self.statistics[\"delay_since_first_attempt\"] = retry_state.seconds_since_start\n        if self.stop(retry_state):\n            if self.retry_error_callback:\n                return self.retry_error_callback(retry_state)\n            retry_exc = self.retry_error_cls(fut)\n            if self.reraise:\n                raise retry_exc.reraise()\n>           raise retry_exc from fut.exception()\nE           tenacity.RetryError: RetryError[<Future at 0x7fb5931c1af0 state=finished raised RateLimitError>]\n\n/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/tenacity/__init__.py:326: RetryError","flaky":false,"newFailed":false,"newBroken":false,"newPassed":false,"retriesCount":0,"retriesStatusChange":false,"beforeStages":[{"name":"event_loop_policy","time":{"start":1713491339643,"stop":1713491339643,"duration":0},"status":"passed","steps":[],"attachments":[],"parameters":[],"shouldDisplayMessage":false,"stepsCount":0,"attachmentsCount":0,"hasContent":false,"attachmentStep":false}],"testStage":{"description":"\n    多轮对话\n    ","status":"broken","statusMessage":"tenacity.RetryError: RetryError[<Future at 0x7fb5931c1af0 state=finished raised RateLimitError>]","statusTrace":"self = <Retrying object at 0x7fb58363a6a0 (stop=<tenacity.stop.stop_after_attempt object at 0x7fb583631550>, wait=<tenacity.w...0x7fb583622b50>, before=<function before_nothing at 0x7fb5855b2ee0>, after=<function after_nothing at 0x7fb5855b91f0>)>\nfn = <function OpenAIBaseHelper.chat at 0x7fb583636310>\nargs = (<mypy.lib.llm.kimi.helper.KimiHelper object at 0x7fb57dbb0520>,)\nkwargs = {'use_stream': True}\nretry_state = <RetryCallState 140417835205680: attempt #5; slept for 36.0; last result: failed (RateLimitError Error code: 429 - {'e...g5g111fix4h1> request reached max requset: 3, please try again after 1 seconds', 'type': 'rate_limit_reached_error'}})>\ndo = <tenacity.DoAttempt object at 0x7fb5931c10d0>\n\n    def __call__(\n        self,\n        fn: t.Callable[..., WrappedFnReturnT],\n        *args: t.Any,\n        **kwargs: t.Any,\n    ) -> WrappedFnReturnT:\n        self.begin()\n    \n        retry_state = RetryCallState(retry_object=self, fn=fn, args=args, kwargs=kwargs)\n        while True:\n            do = self.iter(retry_state=retry_state)\n            if isinstance(do, DoAttempt):\n                try:\n>                   result = fn(*args, **kwargs)\n\n/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/tenacity/__init__.py:382: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nmypy/lib/llm/core/helper.py:83: in chat\n    response = self._client.chat.completions.create(\n/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/openai/_utils/_utils.py:277: in wrapper\n    return func(*args, **kwargs)\n/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/openai/resources/chat/completions.py:581: in create\n    return self._post(\n/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/openai/_base_client.py:1232: in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/openai/_base_client.py:921: in request\n    return self._request(\n/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/openai/_base_client.py:997: in _request\n    return self._retry_request(\n/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/openai/_base_client.py:1045: in _retry_request\n    return self._request(\n/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/openai/_base_client.py:997: in _request\n    return self._retry_request(\n/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/openai/_base_client.py:1045: in _retry_request\n    return self._request(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x7fb593251e50>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        remaining_retries: int | None,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        self._prepare_options(options)\n    \n        retries = self._remaining_retries(remaining_retries, options)\n        request = self._build_request(options)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        try:\n            response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n        except httpx.TimeoutException as err:\n            log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n            if retries > 0:\n                return self._retry_request(\n                    options,\n                    cast_to,\n                    retries,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising timeout error\")\n            raise APITimeoutError(request=request) from err\n        except Exception as err:\n            log.debug(\"Encountered Exception\", exc_info=True)\n    \n            if retries > 0:\n                return self._retry_request(\n                    options,\n                    cast_to,\n                    retries,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising connection error\")\n            raise APIConnectionError(request=request) from err\n    \n        log.debug(\n            'HTTP Request: %s %s \"%i %s\"', request.method, request.url, response.status_code, response.reason_phrase\n        )\n    \n        try:\n            response.raise_for_status()\n        except httpx.HTTPStatusError as err:  # thrown on 4xx and 5xx status code\n            log.debug(\"Encountered httpx.HTTPStatusError\", exc_info=True)\n    \n            if retries > 0 and self._should_retry(err.response):\n                err.response.close()\n                return self._retry_request(\n                    options,\n                    cast_to,\n                    retries,\n                    err.response.headers,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                )\n    \n            # If the response is streamed then we need to explicitly read the response\n            # to completion before attempting to access the response text.\n            if not err.response.is_closed:\n                err.response.read()\n    \n            log.debug(\"Re-raising status error\")\n>           raise self._make_status_error_from_response(err.response) from None\nE           openai.RateLimitError: Error code: 429 - {'error': {'message': 'Your account co2mu8o3r072r8fgnkv0<ak-erfxkb8jg5g111fix4h1> request reached max requset: 3, please try again after 1 seconds', 'type': 'rate_limit_reached_error'}}\n\n/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/openai/_base_client.py:1012: RateLimitError\n\nThe above exception was the direct cause of the following exception:\n\nuse_stream_response = True\n\n    @pytest.mark.parametrize(\n        'use_stream_response', [True, False],\n        ids=['流式响应', '直接响应'])\n    def test_kimi_multi_chat(use_stream_response):\n        \"\"\"\n        多轮对话\n        \"\"\"\n        api = KimiHelper()\n    \n        system_set = '你是一个数学家'\n        user_asks = ['1888 乘以 3212等于多少', '继续减去 1314 后是多少']\n        # step1. 预设角色\n        api.ask_helper.add_ask_message(\n            message=system_set,\n            role=api.ask_helper.ROLE_SYSTEM)\n        # step2. 开始问\n        for ask_msg in user_asks:\n            api.ask_helper.add_ask_message(\n                message=ask_msg,\n                role=api.ask_helper.ROLE_USER)\n    \n            # step3. 像 AI 提问\n>           api.chat(use_stream=use_stream_response)\n\nmypy/tests/lib/llm/kimi/test_kimi.py:43: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/tenacity/__init__.py:289: in wrapped_f\n    return self(f, *args, **kw)\n/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/tenacity/__init__.py:379: in __call__\n    do = self.iter(retry_state=retry_state)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <Retrying object at 0x7fb58363a6a0 (stop=<tenacity.stop.stop_after_attempt object at 0x7fb583631550>, wait=<tenacity.w...0x7fb583622b50>, before=<function before_nothing at 0x7fb5855b2ee0>, after=<function after_nothing at 0x7fb5855b91f0>)>\nretry_state = <RetryCallState 140417835205680: attempt #5; slept for 36.0; last result: failed (RateLimitError Error code: 429 - {'e...g5g111fix4h1> request reached max requset: 3, please try again after 1 seconds', 'type': 'rate_limit_reached_error'}})>\n\n    def iter(self, retry_state: \"RetryCallState\") -> t.Union[DoAttempt, DoSleep, t.Any]:  # noqa\n        fut = retry_state.outcome\n        if fut is None:\n            if self.before is not None:\n                self.before(retry_state)\n            return DoAttempt()\n    \n        is_explicit_retry = fut.failed and isinstance(fut.exception(), TryAgain)\n        if not (is_explicit_retry or self.retry(retry_state)):\n            return fut.result()\n    \n        if self.after is not None:\n            self.after(retry_state)\n    \n        self.statistics[\"delay_since_first_attempt\"] = retry_state.seconds_since_start\n        if self.stop(retry_state):\n            if self.retry_error_callback:\n                return self.retry_error_callback(retry_state)\n            retry_exc = self.retry_error_cls(fut)\n            if self.reraise:\n                raise retry_exc.reraise()\n>           raise retry_exc from fut.exception()\nE           tenacity.RetryError: RetryError[<Future at 0x7fb5931c1af0 state=finished raised RateLimitError>]\n\n/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/tenacity/__init__.py:326: RetryError","steps":[],"attachments":[{"uid":"be0839a21b0fecab","name":"log","source":"be0839a21b0fecab.txt","type":"text/plain","size":1674}],"parameters":[],"shouldDisplayMessage":true,"stepsCount":0,"attachmentsCount":1,"hasContent":true,"attachmentStep":false},"afterStages":[],"labels":[{"name":"parentSuite","value":"lib.llm.kimi"},{"name":"suite","value":"test_kimi"},{"name":"host","value":"fv-az799-397"},{"name":"thread","value":"7152-MainThread"},{"name":"framework","value":"pytest"},{"name":"language","value":"cpython3"},{"name":"package","value":"lib.llm.kimi.test_kimi"},{"name":"resultFormat","value":"allure2"}],"parameters":[{"name":"use_stream_response","value":"True"}],"links":[],"hidden":false,"retry":false,"extra":{"severity":"normal","retries":[],"categories":[{"name":"Test defects","matchedStatuses":[],"flaky":false}],"tags":[]},"source":"fae15ff328fa2436.json","parameterValues":["True"]}