{"uid":"195f9fa74d81ebf","name":"test_ai_chat","fullName":"apps.ai_chat.test_ai_chat#test_ai_chat","historyId":"eb3cd6a1e01ed55e6ea6780e2933ec3d","time":{"start":1714014231623,"stop":1714014352728,"duration":121105},"status":"broken","statusMessage":"tenacity.RetryError: RetryError[<Future at 0x7faa933b6820 state=finished raised RateLimitError>]","statusTrace":"self = <Retrying object at 0x7faaa9b387f0 (stop=<tenacity.stop.stop_after_attempt object at 0x7faaa9b386d0>, wait=<tenacity.w...0x7faaa9b38640>, before=<function before_nothing at 0x7faaa9b28670>, after=<function after_nothing at 0x7faaa9b28940>)>\nfn = <function OpenAIBaseHelper.chat at 0x7faaa9b39d30>\nargs = (<mypy.lib.llm.kimi.helper.KimiHelper object at 0x7faa934acd60>, True)\nkwargs = {}\nretry_state = <RetryCallState 140371014996368: attempt #5; slept for 36.0; last result: failed (RateLimitError Error code: 429 - {'e...g5g111fix4h1> request reached max request: 3, please try again after 1 seconds', 'type': 'rate_limit_reached_error'}})>\ndo = <tenacity.DoAttempt object at 0x7faa933bae50>\n\n    def __call__(\n        self,\n        fn: t.Callable[..., WrappedFnReturnT],\n        *args: t.Any,\n        **kwargs: t.Any,\n    ) -> WrappedFnReturnT:\n        self.begin()\n    \n        retry_state = RetryCallState(retry_object=self, fn=fn, args=args, kwargs=kwargs)\n        while True:\n            do = self.iter(retry_state=retry_state)\n            if isinstance(do, DoAttempt):\n                try:\n>                   result = fn(*args, **kwargs)\n\n/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/tenacity/__init__.py:382: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nmypy/lib/llm/core/helper.py:102: in chat\n    response = self._client.chat.completions.create(model=self._model,\n/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/openai/_utils/_utils.py:277: in wrapper\n    return func(*args, **kwargs)\n/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/openai/resources/chat/completions.py:579: in create\n    return self._post(\n/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/openai/_base_client.py:1232: in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/openai/_base_client.py:921: in request\n    return self._request(\n/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/openai/_base_client.py:997: in _request\n    return self._retry_request(\n/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/openai/_base_client.py:1045: in _retry_request\n    return self._request(\n/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/openai/_base_client.py:997: in _request\n    return self._retry_request(\n/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/openai/_base_client.py:1045: in _retry_request\n    return self._request(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x7faa934c8e20>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        remaining_retries: int | None,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        self._prepare_options(options)\n    \n        retries = self._remaining_retries(remaining_retries, options)\n        request = self._build_request(options)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        try:\n            response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n        except httpx.TimeoutException as err:\n            log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n            if retries > 0:\n                return self._retry_request(\n                    options,\n                    cast_to,\n                    retries,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising timeout error\")\n            raise APITimeoutError(request=request) from err\n        except Exception as err:\n            log.debug(\"Encountered Exception\", exc_info=True)\n    \n            if retries > 0:\n                return self._retry_request(\n                    options,\n                    cast_to,\n                    retries,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising connection error\")\n            raise APIConnectionError(request=request) from err\n    \n        log.debug(\n            'HTTP Request: %s %s \"%i %s\"', request.method, request.url, response.status_code, response.reason_phrase\n        )\n    \n        try:\n            response.raise_for_status()\n        except httpx.HTTPStatusError as err:  # thrown on 4xx and 5xx status code\n            log.debug(\"Encountered httpx.HTTPStatusError\", exc_info=True)\n    \n            if retries > 0 and self._should_retry(err.response):\n                err.response.close()\n                return self._retry_request(\n                    options,\n                    cast_to,\n                    retries,\n                    err.response.headers,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                )\n    \n            # If the response is streamed then we need to explicitly read the response\n            # to completion before attempting to access the response text.\n            if not err.response.is_closed:\n                err.response.read()\n    \n            log.debug(\"Re-raising status error\")\n>           raise self._make_status_error_from_response(err.response) from None\nE           openai.RateLimitError: Error code: 429 - {'error': {'message': 'Your account co2mu8o3r072r8fgnkv0<ak-erfxkb8jg5g111fix4h1> request reached max request: 3, please try again after 1 seconds', 'type': 'rate_limit_reached_error'}}\n\n/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/openai/_base_client.py:1012: RateLimitError\n\nThe above exception was the direct cause of the following exception:\n\nmock_input = <MagicMock name='input' id='140370599239248'>\n\n    @mock.patch('builtins.input')\n    def test_ai_chat(mock_input):\n        # mock 多轮 input\n        mock_input.side_effect = ['质量管理', '不良事件', '0']\n    \n        with pytest.raises(SystemExit):\n            setup_prompt = NursePPT.role_define(subject='建筑')\n>           main(setup_prompt)\n\nmypy/tests/apps/ai_chat/test_ai_chat.py:17: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nmypy/apps/ai_chat/main.py:20: in main\n    api = init_api()\nmypy/apps/ai_chat/main.py:13: in init_api\n    api = LLMHelper(say_hello=True)\nmypy/lib/llm/api.py:49: in __init__\n    self.interactive_chat(Common.hello)\nmypy/lib/llm/api.py:65: in interactive_chat\n    return self._context_api.chat(use_stream)\nmypy/lib/llm/kimi/helper.py:16: in chat\n    return super().chat(use_stream)\n/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/tenacity/__init__.py:289: in wrapped_f\n    return self(f, *args, **kw)\n/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/tenacity/__init__.py:379: in __call__\n    do = self.iter(retry_state=retry_state)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <Retrying object at 0x7faaa9b387f0 (stop=<tenacity.stop.stop_after_attempt object at 0x7faaa9b386d0>, wait=<tenacity.w...0x7faaa9b38640>, before=<function before_nothing at 0x7faaa9b28670>, after=<function after_nothing at 0x7faaa9b28940>)>\nretry_state = <RetryCallState 140371014996368: attempt #5; slept for 36.0; last result: failed (RateLimitError Error code: 429 - {'e...g5g111fix4h1> request reached max request: 3, please try again after 1 seconds', 'type': 'rate_limit_reached_error'}})>\n\n    def iter(self, retry_state: \"RetryCallState\") -> t.Union[DoAttempt, DoSleep, t.Any]:  # noqa\n        fut = retry_state.outcome\n        if fut is None:\n            if self.before is not None:\n                self.before(retry_state)\n            return DoAttempt()\n    \n        is_explicit_retry = fut.failed and isinstance(fut.exception(), TryAgain)\n        if not (is_explicit_retry or self.retry(retry_state)):\n            return fut.result()\n    \n        if self.after is not None:\n            self.after(retry_state)\n    \n        self.statistics[\"delay_since_first_attempt\"] = retry_state.seconds_since_start\n        if self.stop(retry_state):\n            if self.retry_error_callback:\n                return self.retry_error_callback(retry_state)\n            retry_exc = self.retry_error_cls(fut)\n            if self.reraise:\n                raise retry_exc.reraise()\n>           raise retry_exc from fut.exception()\nE           tenacity.RetryError: RetryError[<Future at 0x7faa933b6820 state=finished raised RateLimitError>]\n\n/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/tenacity/__init__.py:326: RetryError","flaky":false,"newFailed":false,"newBroken":false,"newPassed":false,"retriesCount":0,"retriesStatusChange":false,"beforeStages":[{"name":"event_loop_policy","time":{"start":1714014231622,"stop":1714014231622,"duration":0},"status":"passed","steps":[],"attachments":[],"parameters":[],"shouldDisplayMessage":false,"stepsCount":0,"attachmentsCount":0,"hasContent":false,"attachmentStep":false}],"testStage":{"status":"broken","statusMessage":"tenacity.RetryError: RetryError[<Future at 0x7faa933b6820 state=finished raised RateLimitError>]","statusTrace":"self = <Retrying object at 0x7faaa9b387f0 (stop=<tenacity.stop.stop_after_attempt object at 0x7faaa9b386d0>, wait=<tenacity.w...0x7faaa9b38640>, before=<function before_nothing at 0x7faaa9b28670>, after=<function after_nothing at 0x7faaa9b28940>)>\nfn = <function OpenAIBaseHelper.chat at 0x7faaa9b39d30>\nargs = (<mypy.lib.llm.kimi.helper.KimiHelper object at 0x7faa934acd60>, True)\nkwargs = {}\nretry_state = <RetryCallState 140371014996368: attempt #5; slept for 36.0; last result: failed (RateLimitError Error code: 429 - {'e...g5g111fix4h1> request reached max request: 3, please try again after 1 seconds', 'type': 'rate_limit_reached_error'}})>\ndo = <tenacity.DoAttempt object at 0x7faa933bae50>\n\n    def __call__(\n        self,\n        fn: t.Callable[..., WrappedFnReturnT],\n        *args: t.Any,\n        **kwargs: t.Any,\n    ) -> WrappedFnReturnT:\n        self.begin()\n    \n        retry_state = RetryCallState(retry_object=self, fn=fn, args=args, kwargs=kwargs)\n        while True:\n            do = self.iter(retry_state=retry_state)\n            if isinstance(do, DoAttempt):\n                try:\n>                   result = fn(*args, **kwargs)\n\n/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/tenacity/__init__.py:382: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nmypy/lib/llm/core/helper.py:102: in chat\n    response = self._client.chat.completions.create(model=self._model,\n/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/openai/_utils/_utils.py:277: in wrapper\n    return func(*args, **kwargs)\n/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/openai/resources/chat/completions.py:579: in create\n    return self._post(\n/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/openai/_base_client.py:1232: in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/openai/_base_client.py:921: in request\n    return self._request(\n/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/openai/_base_client.py:997: in _request\n    return self._retry_request(\n/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/openai/_base_client.py:1045: in _retry_request\n    return self._request(\n/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/openai/_base_client.py:997: in _request\n    return self._retry_request(\n/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/openai/_base_client.py:1045: in _retry_request\n    return self._request(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x7faa934c8e20>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        remaining_retries: int | None,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        self._prepare_options(options)\n    \n        retries = self._remaining_retries(remaining_retries, options)\n        request = self._build_request(options)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        try:\n            response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n        except httpx.TimeoutException as err:\n            log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n            if retries > 0:\n                return self._retry_request(\n                    options,\n                    cast_to,\n                    retries,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising timeout error\")\n            raise APITimeoutError(request=request) from err\n        except Exception as err:\n            log.debug(\"Encountered Exception\", exc_info=True)\n    \n            if retries > 0:\n                return self._retry_request(\n                    options,\n                    cast_to,\n                    retries,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising connection error\")\n            raise APIConnectionError(request=request) from err\n    \n        log.debug(\n            'HTTP Request: %s %s \"%i %s\"', request.method, request.url, response.status_code, response.reason_phrase\n        )\n    \n        try:\n            response.raise_for_status()\n        except httpx.HTTPStatusError as err:  # thrown on 4xx and 5xx status code\n            log.debug(\"Encountered httpx.HTTPStatusError\", exc_info=True)\n    \n            if retries > 0 and self._should_retry(err.response):\n                err.response.close()\n                return self._retry_request(\n                    options,\n                    cast_to,\n                    retries,\n                    err.response.headers,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                )\n    \n            # If the response is streamed then we need to explicitly read the response\n            # to completion before attempting to access the response text.\n            if not err.response.is_closed:\n                err.response.read()\n    \n            log.debug(\"Re-raising status error\")\n>           raise self._make_status_error_from_response(err.response) from None\nE           openai.RateLimitError: Error code: 429 - {'error': {'message': 'Your account co2mu8o3r072r8fgnkv0<ak-erfxkb8jg5g111fix4h1> request reached max request: 3, please try again after 1 seconds', 'type': 'rate_limit_reached_error'}}\n\n/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/openai/_base_client.py:1012: RateLimitError\n\nThe above exception was the direct cause of the following exception:\n\nmock_input = <MagicMock name='input' id='140370599239248'>\n\n    @mock.patch('builtins.input')\n    def test_ai_chat(mock_input):\n        # mock 多轮 input\n        mock_input.side_effect = ['质量管理', '不良事件', '0']\n    \n        with pytest.raises(SystemExit):\n            setup_prompt = NursePPT.role_define(subject='建筑')\n>           main(setup_prompt)\n\nmypy/tests/apps/ai_chat/test_ai_chat.py:17: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nmypy/apps/ai_chat/main.py:20: in main\n    api = init_api()\nmypy/apps/ai_chat/main.py:13: in init_api\n    api = LLMHelper(say_hello=True)\nmypy/lib/llm/api.py:49: in __init__\n    self.interactive_chat(Common.hello)\nmypy/lib/llm/api.py:65: in interactive_chat\n    return self._context_api.chat(use_stream)\nmypy/lib/llm/kimi/helper.py:16: in chat\n    return super().chat(use_stream)\n/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/tenacity/__init__.py:289: in wrapped_f\n    return self(f, *args, **kw)\n/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/tenacity/__init__.py:379: in __call__\n    do = self.iter(retry_state=retry_state)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <Retrying object at 0x7faaa9b387f0 (stop=<tenacity.stop.stop_after_attempt object at 0x7faaa9b386d0>, wait=<tenacity.w...0x7faaa9b38640>, before=<function before_nothing at 0x7faaa9b28670>, after=<function after_nothing at 0x7faaa9b28940>)>\nretry_state = <RetryCallState 140371014996368: attempt #5; slept for 36.0; last result: failed (RateLimitError Error code: 429 - {'e...g5g111fix4h1> request reached max request: 3, please try again after 1 seconds', 'type': 'rate_limit_reached_error'}})>\n\n    def iter(self, retry_state: \"RetryCallState\") -> t.Union[DoAttempt, DoSleep, t.Any]:  # noqa\n        fut = retry_state.outcome\n        if fut is None:\n            if self.before is not None:\n                self.before(retry_state)\n            return DoAttempt()\n    \n        is_explicit_retry = fut.failed and isinstance(fut.exception(), TryAgain)\n        if not (is_explicit_retry or self.retry(retry_state)):\n            return fut.result()\n    \n        if self.after is not None:\n            self.after(retry_state)\n    \n        self.statistics[\"delay_since_first_attempt\"] = retry_state.seconds_since_start\n        if self.stop(retry_state):\n            if self.retry_error_callback:\n                return self.retry_error_callback(retry_state)\n            retry_exc = self.retry_error_cls(fut)\n            if self.reraise:\n                raise retry_exc.reraise()\n>           raise retry_exc from fut.exception()\nE           tenacity.RetryError: RetryError[<Future at 0x7faa933b6820 state=finished raised RateLimitError>]\n\n/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/tenacity/__init__.py:326: RetryError","steps":[],"attachments":[{"uid":"3b79f5c6673993f3","name":"log","source":"3b79f5c6673993f3.txt","type":"text/plain","size":84}],"parameters":[],"shouldDisplayMessage":true,"stepsCount":0,"attachmentsCount":1,"hasContent":true,"attachmentStep":false},"afterStages":[],"labels":[{"name":"parentSuite","value":"apps.ai_chat"},{"name":"suite","value":"test_ai_chat"},{"name":"host","value":"fv-az905-603"},{"name":"thread","value":"6448-MainThread"},{"name":"framework","value":"pytest"},{"name":"language","value":"cpython3"},{"name":"package","value":"apps.ai_chat.test_ai_chat"},{"name":"resultFormat","value":"allure2"}],"parameters":[],"links":[],"hidden":false,"retry":false,"extra":{"severity":"normal","retries":[],"categories":[{"name":"Test defects","matchedStatuses":[],"flaky":false}],"tags":[]},"source":"195f9fa74d81ebf.json","parameterValues":[]}