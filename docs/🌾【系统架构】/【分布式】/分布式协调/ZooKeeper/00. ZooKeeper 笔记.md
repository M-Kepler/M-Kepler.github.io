- [参考资料](#参考资料)
- [ZooKeeper](#zookeeper)
  - [简介](#简介)
  - [安装配置](#安装配置)
  - [架构](#架构)
    - [节点类型](#节点类型)
    - [监听机制](#监听机制)
    - [事务机制](#事务机制)
  - [高可用](#高可用)
    - [选举机制](#选举机制)
      - [角色分配](#角色分配)
      - [读写流程](#读写流程)
      - [选举机制](#选举机制-1)
  - [客户端](#客户端)
  - [使用](#使用)
  - [应用](#应用)
    - [为什么需要 zookeeper](#为什么需要-zookeeper)
    - [zookeeper 解决了什么问题](#zookeeper-解决了什么问题)
    - [实现分布式锁](#实现分布式锁)
- [学习过程中的疑问](#学习过程中的疑问)
- [排障指南](#排障指南)
- [《Zookeeper 干货精讲系列》](#zookeeper-干货精讲系列)
  - [文件存储](#文件存储)
  - [数据的一致性](#数据的一致性)
  - [半数原则](#半数原则)
  - [CAP 原则](#cap-原则)
    - [Zookeeper 遵循 CP 原则](#zookeeper-遵循-cp-原则)
  - [Paxos 一致性算法](#paxos-一致性算法)
  - [ZAB 协议](#zab-协议)
  - [ZAB 与 Paxos 算法的区别和联系](#zab-与-paxos-算法的区别和联系)
  - [Zookeeper 角色分配](#zookeeper-角色分配)
  - [Zookeeper 主从选举](#zookeeper-主从选举)
- [其他](#其他)

# 参考资料

[为什么用 ETCD 而不用 ZOOKEEPER？](https://mp.weixin.qq.com/s/bgbwupe7OGQMW9tZpkDyuQ)

# ZooKeeper

zoo keeper: 动物园管理员

## 简介

- 说白了就是一个配置中心，放共享文件的地方（HDFS：分布式文件系统），这个可操作性就大了，任何需要进行共享的数据都可以放到这个 文件 里；但是比较适用于存储元数据，比较小的数据；你不能把几十个 G 的数据都存到里面去吧，

- 要存大数据的话，就去用 HDFS

![alt](https://s2.51cto.com/oss/202101/15/31a9e23cffdadb53cbcb225e8efeed5d.png)

## 安装配置

```sh
# 默认配置文件为 zookeeper/conf/zoo_sample.cfg，需要将其修改为zoo.cfg

conf/zoo.cfg
```

- `tickTime` CS 通信心跳时间

  Zookeeper 服务器之间或客户端与服务器之间维持心跳的时间间隔，也就是每个 tickTime 时间就会发送一个心跳。tickTime 以毫秒为单位。

  ```ini
  tickTime=2000
  ```

- `initLimit` LF 初始通信时限

  集群中的 follower 服务器(F)与 leader 服务器(L)之间初始连接时能容忍的最多心跳数（tickTime 的数量）。

  ```ini
  initLimit=5
  ```

- `syncLimit` LF 同步通信时限

  集群中的 follower 服务器与 leader 服务器之间请求和应答之间能容忍的最多心跳数（tickTime 的数量）。

  ```ini
  syncLimit=2
  ```

- `dataDir` 数据文件目录

  Zookeeper 保存数据的目录，默认情况下，Zookeeper 将写数据的日志文件也保存在这个目录里。

  ```ini
  dataDir=/home/michael/opt/zookeeper/data
  ```

- `clientPort` 客户端连接端口

  客户端连接 Zookeeper 服务器的端口，Zookeeper 会监听这个端口，接受客户端的访问请求。

  ```ini
  clientPort=2181
  ```

- 服务器名称与地址：集群信息（服务器编号，服务器地址，LF 通信端口，选举端口）

  这个配置项的书写格式比较特殊，规则如下：

  ```ini
  # YYY 表示服务节点地址
  # AAA 端口是 zookeeper 服务之间通信的端口
  # BBB 端口是 zookeeper 与其他应用程序通信的端口
  # server.N=YYY:AAA:BBB

  server.1=itcast05:2888:3888
  server.2=itcast06:2888:3888
  server.3=itcast07:2888:3888
  ```

## 架构

### 节点类型

持久节点 `PERSISTENT`、持久且有序节点 `PERSISTENT_SEQUENTIAL`、临时节点 `EPHEMERAL`、临时且有序节点 `EPHEMERAL_SEQUENTIAL`

- `持久节点`

  指一旦这个 ZNode 被创建了，除非主动进行 ZNode 的移除操作，否则 ZNode 将存在

- `临时节点` 生命周期和客户会话绑定

  - 一旦客户端会话失败，那么这个客户端创建的所有临时节点都会被删除。

  - 临时 znode 在会话退出时会自动删除，所以不能在临时节点上创建子节点。另外，虽然临时节点属于某会话，但所有客户端都可以查看、引用它。

  - 临时节点不可以有子节点

  另外，zookeeper 还允许用户为每一个节点添加一个特殊的属性 `SEQUENTIAL`。一旦节点被标记上这个属性，那么在这个节点被创建的时候，zookeeper 会自动在其节点名后面追加上一个整型数字，这个整型数字是由父节点维护的自增数字。

zookeeper 在各种分布式场景下发挥作用的主要方式就是数据节点和监控器我们可以给数据节点赋予任意我们想赋予的含义，来协助我们更好的处理分布式程序中所面临的问题。

### 监听机制

**监听什么？谁来监听？怎么监听？**

监听机制是 Zookeeper 中非常重要的特性，我们基于 Zookeeper 上创建的节点，可以对这些节点绑定监听事件；

比如可以监听节点数据变更、节点删除、子节点状态变更等事件，通过这个事件机制，可以基于 Zookeeper 实现分布式锁、集群管理等功能。

**常见**的监听场景有以下两项：

- 监听 Znode 节点的 **更新变化**

- 监听子节点的 **增减变化**

![alt](https://pic3.zhimg.com/v2-d88bb8f393873725519b40f2f1e53246_r.jpg)

![alt](https://pic1.zhimg.com/v2-8250e2c7d6873378a8780d9c03f46e9c_r.jpg)

### 事务机制

## 高可用

- 顺序一致性

  来自任意特定客户端的更新都会按其发送顺序被提交。

  也就是说，如果一个客户端将 Znode z 的值更新为 a，在之后的操作中，它又将 z 的值更新为 b，则没有客户端能够在看到 z 的值是 b 之后再看到值 a（如果没有其他对 z 的更新）。

- 原子性

  每个更新要么成功，要么失败

  这意味着，如果一个更新失败，则不会有客户端会看到这个更新的结果。

- 单一系统映像

  一个客户端无论连接到哪台服务器，它看到的都是同样的系统视图。

  这意味着，如果一个客户端在同一个会话中连接到一台新的服务器，它所看到的系统状态不会比在之前服务器上所看到的更老。当一台服务器出现故障，导致它的一个客户端需要尝试连接集合体中其他的服务器时，所有滞后于故障服务器的服务器都不会接受该连接请求，除非这些服务器赶上故障服务器。

- 持久性

  更新一旦成功，其结果就会持久存在并且不会被撤销

  这表明更新不会受到服务器故障的影响。

### 选举机制

![alt](https://images2018.cnblogs.com/blog/1375459/201807/1375459-20180717124511855-2132474478.png)

#### 角色分配

**Leader 服务器**

是整个 ZooKeeper 集群工作机制中的核心，其主要工作有以下两个：

- 事务请求的唯一调度和处理者，保证集群事务处理的顺序性。

- 集群内部各服务器的调度者。

**Follewer 服务器**

是 ZooKeeper 集群状态的跟随者，其主要工作有以下三个：

- 处理客户端非事务请求，转发事务请求给 Leader 服务器。

- 参与事务请求 Proposal（提案） 的投票。

- 参与 Leader 选举投票。

**Observer 服务器**

充当了一个观察者的角色；在工作原理上基本和 Follower 一致，唯一的区别在于，它不参与任何形式的投票。

- 可以为 Follower 服务器分担点压力

#### 读写流程

![alt](https://static001.infoq.cn/resource/image/83/31/8321be5a632e03123d977db4cd268331.png)

1. 在 Client 向 Follower 发出一个写请求

2. Follower 把请求转发给 Leader

3. Leader 接收到以后开始发起投票并通知 Follower 进行投票

4. Follower 把投票结果发送给 Leader

5. Leader 将结果汇总后，如果需要写入，则开始写入，同时把写入操作通知给 Follower，然后 commit

6. Follower 把请求结果返回给 Client

#### 选举机制

集群中只有 Leader 可以写，所有机器都可以读，所有写请求都会分配一个 zk 集群全局的唯一递增编号 `zxid`，用来保证各种客户端发起的写请求都是有顺序的。

## 客户端

kafka 中的客户端是 `zookeeper-shell.sh`，正宗的是 `zkCli.sh`，用法都一样的

[ZooKeeper 命令行工具 zkCli.sh](https://www.cnblogs.com/f-ck-need-u/p/9232829.html)

## 使用

## 应用

通过 **监听 + Znode 节点 (持久 / 短暂)** ，ZooKeeper 就可以玩出这么多花样了

[微服务为什么一定要 Zookeeper 呢？](https://zhuanlan.zhihu.com/p/102762433)

### 为什么需要 zookeeper

[微服务为什么一定要 zookeeper](https://zhuanlan.zhihu.com/p/102762433)

![alt](https://pic1.zhimg.com/v2-2dfe4bb28b448c3a623deeda2cabecd8_r.jpg)

如果应用 A 和应用 B 都需要获取某一个配置（比如数据库连接信息，单体架构下两个应用都部署在同一台服务器上），那么可以直接按照配置文件路径读取数据；如果并发进行文件修改，也可以通过文件锁等方式进行同步和互斥

但是在分布式架构下，应用 A 和应用 B 被分别部署到不同的节点，如果还是通过配置文件路径的方式来获取这个配置信息，就要把这份配置文件存放到应用所在的服务器上，如果有 N 个应用需要这个信息，就要把这份配置复制 N 份；而且如果配置有更改，需要同时更新所有节点上的配置；

所以，把配置独立出来，做好解耦，放到某台服务器上，通过推或拉的方式获取都可以；

但是这么一来又会存在单点问题，如果这台存配置的服务器挂了怎么办？所以要搭建集群，集群前还要挂一个负载均衡服务，负载均衡服务还要做成集群的，以免挂了；但是如果数据多了的话，每台集群都保留着重复的信息也不好，有没有必要做成分布式的呢，要保证这个配置中心的可靠性。

### zookeeper 解决了什么问题

[Zookeeper-Zookeeper 可以干什么](https://www.cnblogs.com/yuyijq/p/3424473.html)

### 实现分布式锁

有了 zookeeper 的一致性文件系统，锁的问题变得容易。锁服务可以分为两类，一个是保持独占，另一个是控制时序。

对于第一类，我们将 zookeeper 上的一个 znode 看作是一把锁，通过 createznode 的方式来实现。所有客户端都去创建 /task_lock 节点，最终成功创建的那个客户端也即拥有了这把锁。用完删除掉自己创建的 task_lock 节点就释放出锁。

对于第二类， /task_lock 已经预先存在，所有客户端在它下面创建临时顺序编号目录节点，和选 master 一样，编号最小的获得锁，用完删除，依次方便。

# 学习过程中的疑问

其实之前也有在想，分布式系统中的服务发现是怎么做的，其实最简单的就是有个服务 A 在跑着，其他所有服务都在这上面进行注册，也在这上面去查找其他服务的地址信息，这不就解决了吗？

- **既然有监听机制，那怎么保证通知到位了呢？**

- **分布式协调系统？遇到了什么问题需要进行协调？不协调会怎么样？非分布式系统中这些问题是怎么解决的**

  不要被他的原语唬住了，其实就是个小的分布式存储系统，他就是一个存放文件的地方；分布式系统中需要有这么个地方来存放一些共享数据。

- **他本身也是分布式的，怎么处理自身的同步问题**

  问得好

- **为什么分这几种节点类型？分别应对什么问题**

  不是为了解决问题引入这几种节点类型；而是本身支支持是否持久、是否有序这两种属性，是应用者会玩。

- **为什么要设计成目录树的结构，我看也是 k-v 结构存储啊**

- **可以存储多少数据？说是少的数据才存到 zk？有限制吗？如果我存很大的数据呢**

  适合用来存储比较小的元数据，要先用分布式存储，去学习 HDFS

- **上线进行服务注册，下线进行服务注销；但是如果服务挂了呢？怎么办？心跳吗？**

# 排障指南

# 《Zookeeper 干货精讲系列》

- 原来选举结束后的故障转移，也有用到分布式锁啊

## 文件存储

**单机**

- IO

- 单点问题

- 瓶颈

**多机**

[关于 Raid0,Raid1,Raid5,Raid10 的总结](https://www.cnblogs.com/ivictor/p/6099807.html)

- raid0 切片

  RAID 0 提高存储性能的原理是把连续的数据分散到多个磁盘上存取，这样，系统有数据请求就可以被多个磁盘并行的执行，每个磁盘执行属于它自己的那部分数据请求。这种数据上的并行操作可以充分利用总线的带宽，显著提高磁盘整体存取性能。

  ![alt](https://images2015.cnblogs.com/blog/576154/201611/576154-20161124222640518-2080403622.png)

- raid1 全量

  通过磁盘数据镜像实现数据冗余，在成对的独立磁盘上产生互为备份的数据。当原始数据繁忙时，可直接从镜像拷贝中读取数据，因此 RAID 1 可以提高读取性能

  ![alt](https://images2015.cnblogs.com/blog/576154/201611/576154-20161124223115643-890801266.jpg)

- raid5 奇偶校验

  ![alt](https://images2015.cnblogs.com/blog/576154/201611/576154-20161124225904331-1146819836.jpg)

- raid01 就是 raid1 和 raid0 一起

  ![alt](https://images2015.cnblogs.com/blog/576154/201611/576154-20161124232438628-1448311671.jpg)

## 数据的一致性

比如我有一个操作需要同步到集群中的所有节点上

结合事务隔离级别一起看：读未提交、读已提交、可重复读、串行化

**强一致性**

所有的读写操作都按照全局时钟下的是顺序执行，且任何时刻西安成都渠道的数据都是一样的；

更新一次数据，所有的存储节点都要更新数据；而且必须等待所有节点更新完成才可以继续进行读写操作；

写入数据的时候，当前节点不能被读取

**弱一致性、最终一致性**

> 比如说国家颁布政策时，也是一个个地方慢慢推行，最终全国都实施。

读取数据的时候，不需要是最新的，只要能读取到就行；

不能保证任何一次读都能读到最近一次写入的数据，但能保证最终可以读取到写入的数据（zookeeper 很快，毫秒级别就完成同步了）

集群认为，半数以上的机器存储成功就认为是成功了；所以搭建集群的时候，尽量选择奇数个节点

效率高，但是数据不安全（即：数据同步没那么快，比如线程 1 操作了数据 A ，但是还没提交上去，线程 2 也来操作数据 A，看到的还是线程 1 修改前的数据。正常的话，应该加锁，比如线程 1 操作数据 A 完成之前，需要先加锁，线程 2 想要修改的时候就要等待锁）

**顺序一致性**

多个线程的整体执行可能是无序的，但是对于单个线程而言，执行是有序的；要保证任何一次肚兜能读到最近一次写入的数据；

任何一次读都能督导某个数据的最近一次写的数据；

系统的所有进程的顺序一致，而且合理的；即不需要和全局时钟下的顺序一直，错的话一起错，对的话一起对

## 半数原则

和 MySQL 主从同步差不多，我不可能等所有的从节点全同步完成才算完成，MySQL 的话，有一个节点同步完成就行了

集群同步，如果采用最终一致性策略，则是超过半数节点同步完成就算完成。

为什么强调超过半数呢？

把 366 个人放在一起，肯定有 2 个是是同年同月同日生的；

用 3 个抽屉装 4 个苹果，肯定有一个抽屉里有 2 个苹果

比如你有红黄蓝三双袜子，你只要拿 4 只袜子，就可以肯定拿到同颜色的一双袜子

基于以上“抽屉理论”，所以才强调超过半数

所以搭建集群的时候，才会选择奇数个节点数，是为了解决平票问题

## CAP 原则

> 鱼和熊掌不可兼得

CAP 指的是，以下三个要素，最多只能同时实现两点，不可能三者兼顾

- `Consistency 一致性`

  - 每次读取都可以收到最新的写入或错误

  - 所有节点在同一时间具有相同的数据

- `Avaliability 可用性`

  - 在一个固定时间段内，每个请求都会收到一个不是错误的响应（一般都是一台机器）

  - 保证每个请求不管成功或者失败都有响应

- `Partition tolerance 分区容忍性`

  - 节点之间的网络丢弃（或延迟）了任意数量的数据，系统仍可继续运行（即有进行数据备份）

  - 系统中任意信息的丢失或失败不会影响系统的继续运作

### Zookeeper 遵循 CP 原则

即任何时候对 Zookeeper 的访问请求能得到一致的数据结果，同时系统对网络分割具备容错性，但是 Zookeeper 不能保证每次服务请求都是可达的。

从 Zookeeper 的实际应用情况来看，在使用 Zookeeper 获取服务列表时，如果此时的 Zookeeper 集群中的 Leader 宕机了，该集群就要进行 Leader 的选举，又或者 Zookeeper 集群中半数以上服务器节点不可用（例如有三个节点，如果节点一检测到节点三挂了 ，节点二也检测到节点三挂了，那这个节点才算是真的挂了），那么将无法处理该请求。所以说，Zookeeper 不能保证服务可用性。

当然，在大多数分布式环境中，尤其是涉及到数据存储的场景，数据一致性应该是首先被保证的，这也是 Zookeeper 设计紧遵 CP 原则的另一个原因。

但是对于服务发现来说，情况就不太一样了，针对同一个服务，即使注册中心的不同节点保存的服务提供者信息不尽相同，也并不会造成灾难性的后果。

因为对于服务消费者来说，能消费才是最重要的，消费者虽然拿到可能不正确的服务实例信息后尝试消费一下，也要胜过因为无法获取实例信息而不去消费，导致系统异常要好（淘宝的双十一，京东的 618 就是紧遵 AP 的最好参照）。

当 master 节点因为网络故障与其他节点失去联系时，剩余节点会重新进行 leader 选举。问题在于，选举 leader 的时间太长，30~120s，而且选举期间整个 zk 集群都是不可用的，这就导致在选举期间注册服务瘫痪。

在云部署环境下， 因为网络问题使得 zk 集群失去 master 节点是大概率事件，虽然服务能最终恢复，但是漫长的选举事件导致注册长期不可用是不能容忍的。

## Paxos 一致性算法

一致性算法，用于保证分布式系统中，每个节点都顺序执行相同的操作序列，在每个指令上执行一致性算法，能够保证最终每个节点的数据是一致的。

有多个节点，就会存在节点间同行的问题，存在这两种节点通信模型：共享内存、消息传递，paxos 是基于消息传递的通信模型

## ZAB 协议

ZAB（atomic broadcast protocol） 原子广播协议，是为 zookeeper 专门设计的一种支持“崩溃恢复”的原子广播协议。

ZAB 协议包含两种基本的模式：**崩溃恢复、消息广播**

当整个 zookeeper 集群刚刚启动或者 Leader 服务器宕机、重启或者网络故障导致不存在过半的服务器与 Leader 服务器保持正常通信时，所有进程（服务器）进入崩溃恢复模式；

首先选举产生新的 Leader 服务器，然后集群中 Follower 服务器开始与新的 Leader 服务器进行数据同步，当集群中超过半数机器与该 Leader 服务器完成数据同步之后，退出恢复模式进入消息广播模式；

Leader 服务器开始接收客户端的事务请求生成事物提案来进行事务请求处理。

## ZAB 与 Paxos 算法的区别和联系

**相同点：**

两者都存在一个类似于 Leader 进程的角色，由其负责协调多个 Follower 进程的运行

Leader 进程都会等待超过半数的 Follower 做出正确的反馈后，才会将一个提案进行提交

ZAB 协议中，每个 Proposal（提案） 中都包含一个 epoch（时代、时期、纪元） 值来代表当前的 Leader 周期，Paxos 中名字为 Ballot

**不同点：**

ZAB(ZooKeeper Atomic Broadcast) 用来构建高可用的分布式数据主备系统（Zookeeper），Paxos 是用来构建分布式一致性状态机系统。

而 Paxos 算法与 ZAB 协议不同的是，Paxos 算法的发起者可以是一个或多个。当集群中的 Acceptor 服务器中的大多数可以执行会话请求后，提议者服务器只负责发送提交指令，事务的执行实际发生在 Acceptor 服务器。

这与 ZooKeeper 服务器上事务的执行发生在 Leader 服务器上不同。Paxos 算法在数据同步阶段，是多台 Acceptor 服务器作为数据源同步给集群中的多台 Learner 服务器，而 ZooKeeper 则是单台 Leader 服务器作为数据源同步给集群中的其他角色服务器。

**注意：**

ZAB 是在 Paxos 的基础上改进和演变过来的。提议者（Proposer）、决策者（Acceptor）、决策学习者（Learner）

## Zookeeper 角色分配

**对应 Poxos 算法**

- 小岛： zookeeper 集群

- 总统： zookeeper Leader

  - 集群中所有的写数据指令都必须由总统发出

  - 总统是由议员投票产生的

- 议员： zookeeper Learner

  接收客户端请求

  - 查询直接返回结果（有可能数据不一致，查询到的不是最终结果，就像你参加秒杀活动，看到的可能不是最终的记过，但是会慢慢把最终结果同步过来）

  - 写入数据，先将数据写入到当前节点

    - 发送消息给总统，总统将修改数据的指令发送给其他节点

    - 其他节点接收指令后，开始修改数据，修改完成给总统返回成功的消息

    - 当总统发现超过半数的人都已经修改成功，就认为修改成功了

    - 并将消息传递给接受客户端修改数据请求的节点，节点把消息返回给客户端，说明数据更新完成。

- 提议： ZNode Change

  - 客户端的提议会被封装成一个节点，挂载到一个 zookeeper 维护的目录树上

  - 节点值大小不能超过 1M

- 提议编号： Zxid

  会按照数字序列递增，不会减少不会重复；最终达成顺序一致性，就像 Redis 中的 AOF 持久化方式，把操作按顺序记录下来，所有节点的都按顺序执行一遍，那么最终结果也是一致的。

- 正式法令： 所有 Znode 及其数据

  超过半数的人将更新这个数据，就说明数据已经是正式的了。

- 屁民： client

  发送查询、修改请求

## Zookeeper 主从选举

- 根据事务 ID 和 Myid 大小

# 其他

- zookeeper 存放的数据量比较少，所以每个节点存放的都是全量数据
