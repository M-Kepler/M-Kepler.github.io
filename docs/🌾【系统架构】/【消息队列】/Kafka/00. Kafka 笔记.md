- [参考资料](#参考资料)
- [Kafka](#kafka)
  - [简介](#简介)
  - [安装](#安装)
  - [配置](#配置)
    - [server.properties 配置项说明](#serverproperties-配置项说明)
    - [节点分发](#节点分发)
    - [单服务器上配置伪集群（多实例）](#单服务器上配置伪集群多实例)
    - [多服务器上搭建集群](#多服务器上搭建集群)
    - [查看集群状态](#查看集群状态)
    - [容错测试](#容错测试)
  - [使用](#使用)
    - [`kafka-topic.sh`](#kafka-topicsh)
    - [`kafka-consumer-groups.sh`](#kafka-consumer-groupssh)
    - [`kafka-console-producer.sh`](#kafka-console-producersh)
    - [`kafka-console-consumer.sh`](#kafka-console-consumersh)
- [架构](#架构)
  - [broker 代理](#broker-代理)
  - [topic 主题](#topic-主题)
  - [partition 分区](#partition-分区)
  - [logsegment 日志分段](#logsegment-日志分段)
  - [replication 副本](#replication-副本)
  - [leader 主节点](#leader-主节点)
  - [follower 从节点](#follower-从节点)
  - [message 消息](#message-消息)
  - [producers 生产者](#producers-生产者)
  - [consumers 消费者](#consumers-消费者)
  - [consumer group 消费者组](#consumer-group-消费者组)
  - [offset 偏移量](#offset-偏移量)
  - [数据存储](#数据存储)
  - [数据安全性](#数据安全性)
    - [生产者保证 `producer delivery guarantee`](#生产者保证-producer-delivery-guarantee)
    - [ISR 机制](#isr-机制)
    - [数据过期机制](#数据过期机制)
    - [消费者保证 `consumer delivery guarantee`](#消费者保证-consumer-delivery-guarantee)
    - [数据的消费](#数据的消费)
  - [zookeeper](#zookeeper)
  - [kraft](#kraft)
- [监控](#监控)
- [学习过程中的疑问](#学习过程中的疑问)
- [排障指南](#排障指南)
- [其他](#其他)

# 参考资料

- [Kafka 官方中文文档](https://kafka.apachecn.org/)

- [2021 最新版 Kafka 从入门到进阶超详细视频教程](https://www.bilibili.com/video/BV1UU4y147PK?spm_id_from=444.41.top_right_bar_window_custom_collection.content.click)

# Kafka

![alt](https://baiyp.ren/images/mq/kafka/kafka01.png)

## 简介

Kafka 是一个分布式的基于发布 / 订阅模式的消息队列（Message Queue），主要应用与大数据实时处理领域。其主要设计目标如下：

- 以时间复杂度为 `O(1)` 的方式提供消息持久化能力，即使对 TB 级以上数据也能保证常数时间的访问性能

- 高吞吐率。即使在非常廉价的机器上也能做到单机支持 `每秒 100K` 条消息的传输

- 支持 Kafka Server 间的消息分区，及`分布式`消费，同时保证每个 partition 内的消息顺序传输，同时支持离线数据处理和实时数据处理

## 安装

- [安装 kafka](https://blog.csdn.net/haixing1994/article/details/119772277)，一定要注意版本号，3.0 是不一样的

  ```sh
  #####
  # kafka_2.13-3.0.0 以后自带的 zookeeper，不需要单独安装

  # 启动 kafka 前必须先启动 zookeeper
  # 启动 zookeeper
  zookeeper-server-start.sh -daemon config/zookeeper.properties

  # 启动 kafka
  kafka-server-start.sh -daemon config/server.properties
  ```

- [配置 kafka 进程管理服务 `/etc/init.d/kafka`](https://stackoverflow.com/questions/34512287/how-to-automatically-start-kafka-upon-system-startup-in-ubuntu)

- [配置 kafka 进程管理服务 `systemctl`](https://www.jianshu.com/p/abfe6fd849bc)

  **zookeeper.service**

  ```sh
  [Unit]
  Description=Zookeeper service
  After=network.target

  [Service]
  Type=simple
  User=root
  Group=root
  ExecStart=/opt/kafka_2.13-3.2.0/bin/zookeeper-server-start.sh /opt/kafka_2.13-3.2.0/config/zookeeper.properties
  ExecStop=/opt/kafka_2.13-3.2.0/bin/zookeeper-server-stop.sh
  Restart=on-failure

  [Install]
  WantedBy=multi-user.target
  ```

  **kafka.service**

  ```sh
  [Unit]
  Description=Apache Kafka server (broker)
  After=network.target  zookeeper.service

  [Service]
  Type=simple
  User=root
  Group=root
  ExecStart=/opt/kafka_2.13-3.2.0/bin/kafka-server-start.sh /opt/kafka_2.13-3.2.0/config/server.properties
  ExecStop= /opt/kafka_2.13-3.2.0/bin/kafka-server-stop.sh
  Restart=on-failure

  [Install]
  WantedBy=multi-user.target
  ```

## 配置

### server.properties 配置项说明

[server.properties 配置文件参数说明](http://t.zoukankan.com/zouhong-p-12290541.html)

- `broker.id` 当前 Broker 节点 ID

- `log.dirs` 是 kafka 接收消息存放路径

- `listeners=PLAINTEXT://:9092` kafka 服务监听地址和端口

- `zookeeper.connect` 指定连接的 zookeeper 服务地址

- `--bootstrap-server` 指的是目标集群的服务器地址，这个和 `broker-list` 功能是一样的，只不过我们在 console producer 要求用后者

- `host.name`

**Kafka 集群配置外网访问**

[Kafka 集群无法外网访问，报错：`kafka.errors.NoBrokersAvailable: NoBrokersAvailable`](https://www.365seal.com/y/gbvGgA51vA.html)

```ini
# 修改 config/server.properties
# `advertised.listeners` advertised 广告、公布、宣传的意思

advertised.listeners = PLAINTEXT://外网地址:端口
```

### 节点分发

就是在集群外的一台服务器（跳板机）上先配置好，然后 `cp` 或 `scp` 或 `rscp` 同步到集群节点上

- 配置 hostname

  ```sh
  # 在 /etc/hosts 上配置节点地址，后面用 hostname 访问，不用 IP

  10.107.161.41 node-1
  10.107.161.42 node-2
  10.107.161.43 node-3
  ```

- 配置免密登录

  生成公钥私钥（已有的话，就不用）

  ```sh
  # 在 ~/.ssh 下生成公钥私钥
  ssh-keygen

  # 拷贝公钥到各节点
  ssh-copy-id -i ~/.ssh/id_rsa.pub root@node-1
  ssh-copy-id -i ~/.ssh/id_rsa.pub root@node-2
  ssh-copy-id -i ~/.ssh/id_rsa.pub root@node-3

  # 验证下，可以免密登录
  ssh root@node-1

  # 默认用当前用户 $USER 来登录
  ssh node-1

  # 登录时，可能会遇到警告：
  # -bash: warning: setlocale: LC_ALL: cannot change locale (C.UTF-8)
  # https://blog.csdn.net/qq_39545674/article/details/108889030
  ```

- 拷贝配置上去

  ```sh
  cd /opt
  scp -r kafka_2.12-2.4.1/ node-1:$PWD
  ```

- 配置 KAFKA_HOME 环境变量

  ```sh
  vim /etc/profile
  export KAFKA_HOME=/export/server/kafka_2.12-2.4.1
  export PATH=:$PATH:${KAFKA_HOME}

  # 分发到各个节点
  scp /etc/profile node-2:$PWD
  scp /etc/profile node-3:$PWD

  ```

### 单服务器上配置伪集群（多实例）

```sh
# 可以在 /etc/hosts 上配置节点地址
# 127.0.0.1 node-1
# 127.0.0.1 node-2

ping node-1
ping node-2

```

```sh
# 拷贝几份配置，修改 broker.id 和 log.dirs 以及 listeners 的端口
cd /opt/kafka_2.13-3.2.0/
cp server.properties server-1.properties
cp server.properties server-2.properties

# 拷贝几份启动配置，修改其中的 ExecStart 指定启动配置
cd /lib/systemd/system
cp kafka.service kafka-1.service
cp kafka.service kafka-2.service

# 启动集群
systemctl restart zookeeper
systemctl restart kafka
systemctl restart kafka-1
systemctl restart kafka-2

# 创建主题
kafka-topics.sh --create --bootstrap-server node-1:9092 node-2:9093 node-3:9094 --replication-factor 3 --partitions 3 --topic tests

# bootstrap-server 写 node-1:9092 一个地址也是可以的
kafka-topics.sh --describe --bootstrap-server node-1:9092 node-2:9093 node-3:9094 --topic tests

Topic: tests    TopicId: 9UEaUpnYQz6HlboeI2Hsuw PartitionCount: 3       ReplicationFactor: 3    Configs: segment.bytes=1073741824
        Topic: tests    Partition: 0    Leader: 1       Replicas: 1,2,0 Isr: 1,2,0
        Topic: tests    Partition: 1    Leader: 0       Replicas: 0,1,2 Isr: 0,1,2
        Topic: tests    Partition: 2    Leader: 2       Replicas: 2,0,1 Isr: 2,0,1

```

### 多服务器上搭建集群

[kafka 的集群搭建以及 shell 启动命令脚本编写](https://cloud.tencent.com/developer/article/1887278)

[kafka 集群搭建（利用集成 zk 集群）](http://t.zoukankan.com/gered-p-15210271.html)

**/etc/hosts**

```sh
# 可以在 /etc/hosts 上配置节点地址
# 10.107.161.41 node-1
# 10.107.161.42 node-2
# 10.107.161.43 node-3

ping node-1
ping node-2
ping node-3
```

**server.properties**

```sh
# zookeeper.connect：zookeeper连接地址，多个以逗号隔开
zookeeper.connect=node-1:2181,node-2:2181,node-3:2181

```

**zookeeper.properties**

```sh
# 2181 客户端连接端口
# 2182 端口是 zookeeper 服务之间通信的端口
# 2183 端口是 zookeeper 与其他应用程序通信的端口
clientPort=2181
server.1=node-1:2182:2183
server.2=node-2:2182:2183
server.3=node-3:2182:2183

# 创建集群节点标识（不同节点不同 ID）
echo "1" > /tmp/zookeeper/myid

# 防火墙开放 2181 2182 2183 端口
firewall-cmd --zone=public --add-port=2181/tcp --permanent
firewall-cmd --zone=public --add-port=2182/tcp --permanent
firewall-cmd --zone=public --add-port=2183/tcp --permanent

firewall-cmd --reload

# 查看 返回 yes 代表开启成功
firewall-cmd --zone=public --query-port=2181/tcp
firewall-cmd --zone=public --query-port=2182/tcp
firewall-cmd --zone=public --query-port=2183/tcp


# 在 /opt/kafka_2.13-3.2.0/logs/server.log 中可以看到链接了两个节点
# INFO Initiating client connection, connectString=node-1:2181,node-2:2181

# 在 node-1 测试一下能不能连到集群中的节点 node-2
zookeeper-shell.sh 10.107.161.42:2182

# 创建主题
# 如果出现 kafka 启动失败，报错 The Cluster ID DEAdnBfwT2Cj6SNun5Vi4A doesn't match stored clusterId Some(_J80o2MhSvecnxBCUkjChA) in meta.properties
# 需要把 kafka 日志目录下的 meta.properties 删掉

kafka-topics.sh --create --bootstrap-server node-1:9092 node-2:9092 node-3:9092 --replication-factor 3 --partitions 3 --topic test

# 查看主题详情

kafka-topics.sh --describe --bootstrap-server node-1:9092 --topic test
Topic: test     TopicId: 69v5waGtRber4Dynm38E_Q PartitionCount: 3       ReplicationFactor: 3    Configs: segment.bytes=1073741824
        Topic: test     Partition: 0    Leader: 3       Replicas: 3,1,2 Isr: 3,1,2
        Topic: test     Partition: 1    Leader: 1       Replicas: 1,2,3 Isr: 1,2,3
        Topic: test     Partition: 2    Leader: 2       Replicas: 2,3,1 Isr: 2,3,1

```

### 查看集群状态

我想看一下鸡群里是不是所有节点都正确链接进来了

```sh
# 随意连接到 zookeeper 集群中的某个节点
$zookeeper-shell.sh node-1:2181 # node-2:2181 node-3:2181

Connecting to node-1:2181
Welcome to ZooKeeper!
JLine support is disabled

WATCHER::

WatchedEvent state:SyncConnected type:None path:null

# 查看存储目录
ls /
[admin, brokers, cluster, config, consumers, controller, controller_epoch, feature, isr_change_notification, latest_producer_id_block, log_dir_event_notification, zookeeper]

# 查看集群中的节点（这里的 ID 就是上面组建集群时的 /tmp/zookeeper/myid）
ls /brokers/ids
[1, 2, 3]

# 查看节点具体信息
get /brokers/ids/1
{"features":{},"listener_security_protocol_map":{"PLAINTEXT":"PLAINTEXT"},"endpoints":["PLAINTEXT://node-1:9092"],"jmx_port":-1,"port":9092,"host":"node-1","version":5,"timestamp":"1654763163249"}

get /brokers/ids/2
{"features":{},"listener_security_protocol_map":{"PLAINTEXT":"PLAINTEXT"},"endpoints":["PLAINTEXT://node-2:9092"],"jmx_port":-1,"port":9092,"host":"node-2","version":5,"timestamp":"1654763613345"}

get /brokers/ids/3
{"features":{},"listener_security_protocol_map":{"PLAINTEXT":"PLAINTEXT"},"endpoints":["PLAINTEXT://node-3:9092"],"jmx_port":-1,"port":9092,"host":"node-3","version":5,"timestamp":"1654772925381"}

# 可以看到主题
ls /brokers/topics
[test]

# 查看主题具体信息
get /brokers/topics/test
{"partitions":{"0":[3,1,2],"1":[1,2,3],"2":[2,3,1]},"topic_id":"69v5waGtRber4Dynm38E_Q","adding_replicas":{},"removing_replicas":{},"version":3}

```

### 容错测试

```sh
# 查看主题详情
kafka-topics.sh --describe --bootstrap-server node-1:9092 --topic test

Topic: test     TopicId: 69v5waGtRber4Dynm38E_Q PartitionCount: 3       ReplicationFactor: 3    Configs: segment.bytes=1073741824
        Topic: test     Partition: 0    Leader: 3       Replicas: 3,1,2 Isr: 3,1,2
        Topic: test     Partition: 1    Leader: 1       Replicas: 1,2,3 Isr: 1,2,3
        Topic: test     Partition: 2    Leader: 2       Replicas: 2,3,1 Isr: 2,3,1

# kill 掉节点 node-1 的 kafka
kill -9 [pid_of_kafka_in_node-1]

# 查看主题详情
# partition 的 Leader 已切换，并且节点 node-1 不在同步副本集 ISR 中
kafka-topics.sh --describe --bootstrap-server node-2:9092

Topic: test     TopicId: 69v5waGtRber4Dynm38E_Q PartitionCount: 3       ReplicationFactor: 3    Configs: segment.bytes=1073741824
        Topic: test     Partition: 0    Leader: 3       Replicas: 3,1,2 Isr: 3,2
        Topic: test     Partition: 1    Leader: 2       Replicas: 1,2,3 Isr: 2,3
        Topic: test     Partition: 2    Leader: 2       Replicas: 2,3,1 Isr: 2,3

# node-1 节点重新上线，会再次出现在 ISR 中
kafka-topics.sh --describe --bootstrap-server node-2:9092

Topic: test     TopicId: 69v5waGtRber4Dynm38E_Q PartitionCount: 3       ReplicationFactor: 3    Configs: segment.bytes=1073741824
        Topic: test     Partition: 0    Leader: 3       Replicas: 3,1,2 Isr: 3,2,1
        Topic: test     Partition: 1    Leader: 2       Replicas: 1,2,3 Isr: 2,3,1
        Topic: test     Partition: 2    Leader: 2       Replicas: 2,3,1 Isr: 2,3,1

```

## 使用

**3.0+ 新版本命令中的 `--zookeeper-server localhost:2181` 修改为 `--bootstrap-server localhost:9092`**

### `kafka-topic.sh`

- 创建主题

  ```sh
  kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 3 --topic test

  # --create 创建操作
  # --bootstrap-server KAFKA 服务器节点地址
  # --replication-factor 副本数量 <= 所有节点数量（副本数量不能多于 kafka 服务器数量）
  # --topic 主题名字
  # --partitions 分区
  ```

- 查看主题列表

  ```sh
  kafka-topics.sh --list --bootstrap-server localhost:9092
  ```

- 查看主题详细信息

  ```sh

  # 查看topic的详细信息，需要使用describe命令
  kafka-topics.sh --describe --bootstrap-server localhost:9092 --topic test

  # Topic: test     TopicId: O2JxZB2CR-6pnHL-smtMpQ PartitionCount: 3       ReplicationFactor: 2    Configs: segment.bytes=1073741824
  #         Topic: test     Partition: 0    Leader: 1       Replicas: 1,0   Isr: 1,0
  #         Topic: test     Partition: 1    Leader: 0       Replicas: 0,1   Isr: 0,1
  #         Topic: test     Partition: 2    Leader: 1       Replicas: 1,0   Isr: 1,0

  ```

- 删除主题

  [彻底删除 Kafka 中的 Topic（提示：marked for deletion）](https://blog.csdn.net/yuxielea/article/details/105806082)

  ```sh
  kafka-topics.sh --delete --bootstrap-server localhost:9092 --topic test
  ```

### `kafka-consumer-groups.sh`

[选项](https://blog.csdn.net/qq_29116427/article/details/80206125)

- 查看消费组

  ```sh
  kafka-consumer-groups.sh --bootstrap-server localhost:9092 --list

  ```

- 查看消费组 `test-consumer-group` 的详情

  ```sh
  kafka-consumer-groups.sh --new-consumer --bootstrap-server localhost:9092  --group test-consumer-group --describe
  ```

### `kafka-console-producer.sh`

- 启动生产者终端

  ```sh
  # 启动生产者
  kafka-console-producer.sh --bootstrap-server localhost:9092 --topic test
  ```

### `kafka-console-consumer.sh`

- 启动消费者终端

  ```sh
  # 从头开始接收数据，需要添加--from-beginning 参数
  kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginning
  ```

# 架构

[Kafka 的生成者、消费者、broker 的基本概念](https://blog.csdn.net/u010020099/article/details/82290403)

![alt](https://img-blog.csdn.net/20180902105920995?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTAwMjAwOTk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

![alt](https://img-blog.csdnimg.cn/20201227195831601.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L21hcmt5aW5nc2hvdQ==,size_16,color_FFFFFF,t_70)

## broker 代理

缓存代理，Kafka 集群中的一台或多台服务器统称 broker，一个 broker 可以容纳多个 topic

- Broker 没有副本机制，一旦 broker 宕机，该 broker 的消息将都不可用。

- Broker 不保存订阅者的状态，由订阅者自己保存。

- 无状态导致消息的删除成为难题（可能删除的消息正在被订阅），Kafka 采用基于时间的 SLA（服务保证），消息保存一定时间（通常是一星期）后会删除。

- 消费订阅者可以 rewind back 到任意位置重新进行消费，当订阅者故障时，可以选择最小的 offset(id) 进行重新读取消费消息

## topic 主题

Kafka 处理资源的消息源的不同分类；可以理解为一个队列，或者一张数据表，生产者和消费者面向的都是一个 topic

用户只需指定消息的 topic 即可生产或消费数据，**而不需要关系数据存在哪，kafka 内部会帮我们做路由和管理**

每个 topic 至少一个 partition，答案生产者产生数据的时候，根据分配策略（hash 取模），选择分区，将消息 append 追加到指定分区的末尾

## partition 分区

Topic 物理上的分组，为了实现水平扩展，**一个 topic 可以分为多个 partion 发不到多个 broker 上, 每个 partion 是一个有序的队列**。

**partition 中的消息是怎么保证有序的**：每个 partition 在 kafka 存储层面是 append log；任何发布到该 partition 的消息都会被追加到 log 文件末尾，在分区中的每条消息都会按照时间顺序分配到一个 单调递增的顺序编号（分布式全局 ID 的生成），也就是 offset。

partion 中每条消息都会被分配一个有序的 Id(offset)；**如果要严格保证消息的顺序消费，需要把 partition 数目设为 1**

![alt](https://oss-emcsprod-public.modb.pro/wechatSpider/modb_20220328_5242aa1e-ae88-11ec-a71b-fa163eb4f6be.png)

每个消息都有一个自增的编号（图中数字），用来标识顺序和作为偏移量

![alt](https://oss-emcsprod-public.modb.pro/wechatSpider/modb_20220328_525242e4-ae88-11ec-a71b-fa163eb4f6be.png)

不同 partition 中的数据是无序的。比如生产者生产 [1, 2, 3, 4, 5, 6] 消息放入到 topic-test 中，这个 topic 有 p_a 和 p_b 两个 partition；有可能 p_a 中存放的是 [1, 3, 5]，p_b 中存放的是 [2, 4, 6]；消费者订阅该 topic，获取到的消息有可能是 [1, 3, 5, 2, 4, 6] 或者 [2, 4, 6, 1, 3, 5]

也就是说，partition 内部是有序的，但是一个 topic 消息可以分散在多个 partition 中

**partion 数据路由规则**

- 指定 partition ，则直接把消息放到对应的 partition 中

- 没有指定 partition ，但是指定了 key，通过对 key 的 value 进行 hash 选出一个 partition

- partition 和 key 都没指定，则轮询选出一个 partition

## logsegment 日志分段

Log 日志在物理上只是以文件夹的形式存储，而每个 LogSegement 对应磁盘上的一个日志文件和两个索引文件

为了防止 Log 日志过大，Kafka 又引入了日志分段(LogSegment)的概念，将 Log 切分为多个 LogSegement，相当于一个巨型文件被平均分割为一些相对较小的文件，这样也便于消息的查找、维护和清理。这样在做历史数据清理的时候，直接删除旧的 LogSegement 文件就可以了

## replication 副本

为保证集群中的某个节点发生故障时，该节点上的 partition 数据不丢失；副本个数不能超过 broker 个数

备份数设置为 N 表示 **主 + 备 = N**

**kafka 分配 replication 的规则**

基本就是错位在下一个节点上

- 将所有的 broker（假设为 n 个） 和待分配的 partition 排序

- 将第 i 个 partition 分配到第 (i mod n) 个 broker 上

- 将第 i 个 partition 的第 j 个 replication 分配到第 ((i + j) mod n) 个 broker 上

## leader 主节点

leader 负责写入和读取；follower 只负责备份；数据必须从主节点读取（MySQL 是写主节点，读从节点；可能是为了避开主从同步产生的时延问题吧）

**leader 选举**

## follower 从节点

不提供数据读取服务，只能从 leader 读取数据

当 follower 挂掉或同步太慢，leader 会把这个 follower 从 ISR 中删除，重新创建一个 follower

判断 folloer 在线的依据：

- follower 与 zookeeper 有心跳通信

- follower 与 leader 通信不会太慢

## message 消息

通信的基本单位，每个 producer 可以向一个 topic 发布消息。

- Kafka 中的 Message 是以 topic 为基本单位组织的，不同的 topic 之间是相互独立的，每个 topic 又可以分成不同的 partition 每个 partition 储存一部分

- partion 中的每条 Message 包含以下三个属性：

  | 属性          | 类型                  |
  | ------------- | --------------------- |
  | `offset`      | `long`                |
  | `MessageSize` | `int32`               |
  | `data`        | `messages` 的具体内容 |

## producers 生产者

- 消息和数据生成者，向 Kafka 的一个 topic 发布消息的过程叫做 producers

- Producer 将消息发布到指定的 Topic 中，同时 Producer 也能决定将此消息归属于哪个 partition；比如基于 `round-robin`（轮询负载） 方式或者通过其他的一些算法等；

- 异步发送批量发送可以很有效的提高发送效率。kafka producer 的异步发送模式允许进行批量发送，先将消息缓存到内存中，然后一次请求批量发送出去。

- 当 broker 收到消息时，它会返回一个应答。如果消息成功写入，broker 将返回 RecordMetadata 对象（包含 topic，partition 和 offset）；相反，broker 将返回 error。这时 producer 收到 error 会尝试重试发送消息几次，直到 producer 返回 error。

## consumers 消费者

消息和数据消费者，订阅 topic 并处理其发布的消息的过程叫做 consumers，可以订阅多个 topic

- 在 kafka 中，我们可以认为一个 consumer group 是一个 “订阅者”，**`一个 topic 中的每个 partions 只会被其中的一个 consumer 消费，不过一个 consumer 可以消费多个 partitions 中的消息`**

- **注：** Kafka 的设计原理决定，对于一个 topic，同一个 group，不能多于 partition 个数的 consumer 同时消费，否则将意味着某些 consumer 无法得到消息

- 消费者的数量不应该多余分区的数量，因为在一个消费者组中，每个分区至多只能绑定到一个消费者上，即一个消费者可以消费多个分区，`一个分区只能给一个消费者消费`；因此，若一个 group 中的消费者数量大于分区数量的话，多余的消费者将不会收到任何消息。

## consumer group 消费者组

- 每个消费者属于一个特定的消费者组，即使没有指定，也会有一个默认组；

- 多个消费者集中去一起处理某一个 topic 数据，可以更快的提高数据的消费能力

- **整个消费者组共享一组偏移量**，防止数据被重复读取（中间锁吗？），因为一个 topic 有多个 partition

**重平衡 `rebalance`**

- 当新的消费者加入消费组，它会消费一个或多个分区，而这些分区之前是由其他消费者负责的；另外，当消费者离开消费组（比如重启、宕机等）时，它所消费的分区会分配给其他分区。这种现象称为重平衡。

- 重平衡是 Kafka 一个很重要的性质，这个性质保证了高可用和水平扩展。不过也需要注意到，在重平衡期间，所有消费者都不能消费消息，因此会造成整个消费组短暂的不可用。而且，将分区进行重平衡也会导致原来的消费者状态过期，从而导致消费者需要重新更新状态，这段期间也会降低消费性能。

## offset 偏移量

消费者读取数据时，broker 怎么知道这个消费者应该**从那里开始**读取数据呢，而且每个消费者的消费进度都是不一样的；

`zookeeper` 会记录每个消费者的消费进度；

- 为什么不把消费进度保留在 broker 中呢?

  0.9 版本之前 offset 保存在 zk，之后的版本就是保存在本地了

- 那什么时候开始记录或者删除偏移量? 是等消费者反馈成功之后，还是读取之后?

  kafka 也不管这个问题，可以由消费者自行决定什么时候更新进度

  消息最终还是会被删除的，默认生命周期是一周

- 偏移量不会有线程安全问题

- kafka 的存储文件都是按照 `[offset].[kafka]` 来命名的，用 offset 做名字的好处是方便查找

## 数据存储

![alt](https://mmbiz.qpic.cn/mmbiz_png/FrBePKkiazpqJibAKy9cChlumq32YW4MnTLMI8wCs1g8hKEwNLFzr2viakMTy5KQg86ajlpVY7kia116RDxZFApVXQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

```sh
[root@localhost ~]# tree /tmp/kafka-logs-1/

/tmp/kafka-logs-1/
├── cleaner-offset-checkpoint # 存了每个 log 的最后清理 offset
├── log-start-offset-checkpoint # 文件对应 logStartOffset，用来标识日志的起始偏移量
  # kafka 有定时任务负责将所有分区的 logStartOffset 写到该文件
├── meta.properties # broker.id 信息
├── recovery-point-offset-checkpoint # 表示已经刷写到磁盘的记录
    # recoveryPoint 以下的数据都是已经刷到磁盘上的了
├── replication-offset-checkpoint # 用来存储每个 replica 的 HighWatermark 的 (high watermark (HW)
    # 表示已经被 commited 的 message，HW 以下的数据都是各个 replicas 间同步的，一致的。)
├── test-0 # test 为主题名称， 0 表示分区，可以看到主题 test 被分为了 0 1 2 三个分区
│   ├── 00000000000000000000.index # 索引文件
│   ├── 00000000000000000000.log   # 数据文件
│   ├── 00000000000000000000.timeindex
│   ├── leader-epoch-checkpoint
│   └── partition.metadata
├── test-1
│   ├── 00000000000000000000.index
│   ├── 00000000000000000000.log
│   ├── 00000000000000000000.timeindex
│   ├── leader-epoch-checkpoint
│   └── partition.metadata
└── test-2
    ├── 00000000000000000000.index
    ├── 00000000000000000000.log
    ├── 00000000000000000000.timeindex
    ├── leader-epoch-checkpoint
    └── partition.metadata

3 directories, 20 files
```

**稀疏索引（高速读） + 顺序追加日志（高速写）**

主题 `topic` + 分区 `partition` + 分段 `segment` + 索引 `.index`

- topic 是逻辑上的概念，在物理层面上，一个 topic 被分成若干个 partition

- partition 还可以细分为 segment，一个 partition 物理上由多个 segment 组成

  - `log.segment.bytes` 单个 segment 可容纳的最大数据量（默认为 1GB）

  - `log.segment.ms` kafka 在 commit 一未来写满的 segment 前所等待的时间（默认 7 天）

- segment 文件由 `.log` 数据文件和 `.index` 索引文件组成

  - partition 全局的第一个 segment 从 0 开始，后序每个 segment 文件名为上一个 segment 文件最后一条信息的 offset 值

    比如第一个 segment 是 00000000000000000000.index，下一个可能是 00000000000000170410.index，170410 就是上一个 segment 最后一条消息的 offset

  - 数字大小为 64 位，20 位数字字符长度，没有数字用 0 填充

  ```sh
  $kafka-topics.sh --describe --bootstrap-server localhost:9092 --topic test
  Topic: test     TopicId: dTfxj-RoT2K20G1Ls-bejA PartitionCount: 3       ReplicationFactor: 1    Configs: segment.bytes=1073741824
          Topic: test     Partition: 0    Leader: 1       Replicas: 1     Isr: 1
          Topic: test     Partition: 1    Leader: 1       Replicas: 1     Isr: 1
          Topic: test     Partition: 2    Leader: 1       Replicas: 1     Isr: 1

  $ls -l /tmp/kafka-logs/test-0/
  total 20480
  -rw-r--r-- 1 root root 10485760 May 28 10:37 00000000000000000000.index

    # .index 索引文件（初始申请了 10M 的空间）

  -rw-r--r-- 1 root root        0 May 28 10:37 00000000000000000000.log
    ##### .log 数据文件

  -rw-r--r-- 1 root root 10485756 May 28 10:37 00000000000000000000.timeindex
  -rw-r--r-- 1 root root        8 May 28 10:39 leader-epoch-checkpoint
  -rw-r--r-- 1 root root       43 May 28 10:37 partition.metadata
  ```

- 消息像协议一样，都有固定的格式，包括 offset（8bytes)、消息体大小（4 bytes)、crc32（4 bytes）、magic（1 bytes）、attributes（1 bytes）、key length（4 bytes）、key（n bytes）、payload（n bytes） 等等字段，可以确定一条消息的大小，即读取到哪里截止

  ```py
  ConsumerRecord(
      topic='test',
      partition=1,
      offset=0,
      timestamp=1653795084880,
      timestamp_type=0,
      key=None,
      value=b'hello5',
      headers=[],
      checksum=None,
      serialized_key_size=-1,
      serialized_value_size=6,
      serialized_header_size=-1)
  ```

  ![alt](https://img2020.cnblogs.com/blog/1140467/202007/1140467-20200731153526307-2123639447.png)

  为什么没有 `2 3 4 5...`；因为这是个稀疏索引，不会为每条数据都建立索引；

  我感觉就像位图一样，先定位到数据所在的字节，然后定位到数据所在的位

  需要消费 `911` 这条数据的时候，检索过程是这样的：先找到 900 这个索引 `.index` 文件，然后找第 `911 - 900 = 11` 或小于 11（稀疏索引） 的索引；然后通过这条索引的物理位置 1367 开始完后找，找到 911 这条数据

**顺序读和随机读**

顺序 IO 性能比随机 IO 性能强得多；读写连续的内存会更快，所以 kafka 直接申请了 10M 的索引空间

## 数据安全性

```log
producer --> kafka --> consumer
```

生产者把消息发给 kafka，消费者从 kafka 取数据，kafka 自身宕机，或者网络波动等都有可能导致消息丢失，即，没办法保证消息传输和消息处理时原子性的

### 生产者保证 `producer delivery guarantee`

生产商交货保证，生产者确保消息存入到 topic 了；其中的 ACK 机制和 MySQL 的三种主从同步机制（异步复制、半同步复制、全同步复制）很像

`request.required.acks` 选项决定 producer 可以选择是否为数据的写入接收 ack，kafka 有以下三种模式（消费者也一样）：

- 模式: `At most once` 消息可能会丢，当绝不会重复传输

  `acks = 1（默认值）` producer 发送后即为成功，无需等待来自 `broker` 的确认就可以继续发送下一条消息

- 模式: `At least one` 消息绝不会丢，但是可能会重复传输（重复可能会出现问题的，kafka 可不会帮你做去重之类的操作；比如是一条转账消息，你发重复了，这可就出问题了

  `acks = 0` producer 在 ISR 的 **`leader`** 已成功接收到数据，并得到确认后，才发送下一条消息

- 模式: `Exactly one` 每条消息肯定会被传输一次，且仅一次

  `acks = all` leader 与 replicas 的 ack，producer 需要等待 ISR 中 **`所有的 follower`** 都确认接收到数据后，才算一次发送成功，可靠性最高

### ISR 机制

> ISR: in sync replicas 同步列表

类似主备，主节点挂了，备节点顶上；producer 发数据给 leader，follower 从 leader 拉取数据，保证数据同步

这么看，也还是无法避免和 M 有 SQL 主从时延一样的问题

当 follower `replica.lag.time.max.ms = 10000` 毫秒还没过来同步数据，或者主从之间差了 `replica.lag.max.messages = 4000` 条数据时，判断该 folloer 节点为脏节点

虽然这个节点保存的是脏数据，但是 kafka 还是允许这个节点参与到选举中；只是会对他进行降级；选举第一个恢复的节点作为 leader 提供服务，以它的数据为基准，这个措施称为 **`脏 leader 选举`**

### 数据过期机制

无论消息是否被消费，kafka 都会保存所有消息，有两种策略可以删除旧数据

```sh
# 基于时间（默认一个星期）
log.retention.hours = 168

# 基于大小（默认是 1G）
log.retention.bytes = 1073741824
```

### 消费者保证 `consumer delivery guarantee`

如果把 consumer 设置为 auto commit 自动提交偏移量（消费者消费后，要告诉 kafka 偏移量），consumer 一旦读取到数据，就会告诉 kafka 更新偏移量

- `At most one` 读完消息，先 commit 再处理

  如果 consumer 在 commit 之后，还没来得及处理消息就崩溃了，下次重新开始工作后，就无法读取到刚刚已提交而未处理的消息了，**消息丢失**了

- `At least once` 读完消息，先处理再 commit

  如果在处理完成消息，还没来得及 commit 就崩溃了，下次重新开始工作时，还会处理刚才没来得及 commit 的消息，消息就被**重复消费**了

- 如果一定要做到只消费一次，就需要协调 offet 和实际操作的输出；经典的做法就是引入 **两阶段提交**（MySQL 那个）

### 数据的消费

假设一个 topic `设置 2 个 partition`，启动一个 consumer 进程订阅这个 topic，对应的 stream_num 设置为 2，也就是说 `启动两个线程并行处理消息`：

**如果开启自动提交 `auto.commit.enable = true`**

当 consumer 读取到一些数据，但是还没处理完成的时候，刚好到了提交的时间点 commit interval，提交了 offset，接着 consumer 崩掉了；这时已经读取的数据还没处理完，但是已经被 commit 了，偏移量已经更新了，因此也就没机会再被处理了，最终数据丢失

**如果没开启自动提交**

假设 consumer 的两个线程，各自拿了一条数据，并且同时开始处理；此时线程 t1 处理完了 partition1 的数据，手动提交了 offset（`当手动提交的时候，实际上是对这个 consumer 进程所占用的所有partition 进行 commit`，kafka 暂时还没有提供更细粒度的提交方式）；

也就是说即使线程 t2 读取了 partition2 的数据，但是还没处理完，偏移量也会被 t1 提交掉，那就又会出现开启自动提交的情况，如果此时 consumer 崩了，也是会导致数据丢失。

**解决方案一：将多线程问题转为单线程问题**

`开启手动提交，启用和 partition 相同的数目的 consumer 进程数`；

这样就可以保证一个 consumer 进程占用一个 partition，自动提交的时候，不会影响到别的 partition 的 offset；

很明显 partition 数量要和 consumer 数量`严格对应`，要不然还是会出问题

**解决方案二：增加一层缓存**

```sh
# 为了保证事务性，把收到的数据保存起来，消费后再删除，全部消费完再批量提交偏移量
# 但无论如何都有可能出现问题啊
# 比如说从 kafka 读取到了数据，缓存到了队列里；开始业务处理 do do_something，然后更新处理进度 delete from queue
# 这是就有可能在更新处理进度的时间节点崩溃了
# 等服务恢复了，如果先从 queue 中读取数据，那么消息会被重复处理啊

[kafka] -----> consumer -----> [cache to queue] --- [do_something] -----+
   ↑                                                                    ↓
   +---------- [commit offset] <----- [queue is empty] <----- [delete from queue]
```

开启手动提交，另外在 consumer 端再将所有读取到的数据缓存到队列中，当把队列中的数据处理完后，再`批量提交 offset`，这样就可以保证已处理的数据都能被正常 commit

## zookeeper

[KAFKA 依赖 ZOOKEEPER 原因解析及应用场景](https://www.cnblogs.com/dannylinux/articles/10516093.html)

- kafka 在 3.0 版本以前，是通过 zookeeper 来进行集群中统一配置管理；**除了数据文件是存在本机以外，其他一切信息都是存在 zookeeper（所以所有的命令都要指定 zookeeper 地址）**；可以把 zookeeper 当做一个文件管理器

- 在 3.0 版本以后，完全切到 Kafka Raft 模式 了，部署 kafka 集群不再赖一个 [重量级的协调系统 ZooKeeper](https://developer.51cto.com/article/660217.html)

![alt](https://upload-images.jianshu.io/upload_images/22714749-b7ae5b8e0df80374.png)

![alt](https://img2018.cnblogs.com/blog/1577453/201906/1577453-20190624115811350-1213924299.png)

## kraft

[Kafka Raft 模式](https://cloud.tencent.com/developer/article/1840424)

![alt](https://ask.qcloudimg.com/http-save/1009808/u6qap4m7bh.png)

# 监控

# 学习过程中的疑问

- **我现在组了个三节点的集群，因为生产消息的时候需要 `--bootstrap-server` 指定集群地址，如果我只往其中一个节点生产消息会怎么样？消息会落到其他节点吗？其他节点可以看到这个数据吗**

  ```sh
  # 查看主题
  [root@node-1 ~]# kafka-topics.sh --list --bootstrap-server node-1:9092 node-2:9092 node-3:9092
  __consumer_offsets
  json-topic
  msgpack-topic
  my-topic
  test

  # 查看主题详情
  [root@node-1 ~]# kafka-topics.sh --describe --bootstrap-server node-1:9092 --topic my-topic
  Topic: my-topic TopicId: c-b5qgWlT8K8gfG9Lvxdaw PartitionCount: 1       ReplicationFactor: 1    Configs: segment.bytes=1073741824
          Topic: my-topic Partition: 0    Leader: 1       Replicas: 1     Isr: 1

  # 查看主题详情
  [root@node-1 ~]# kafka-topics.sh --describe --bootstrap-server node-1:9092 --topic msgpack-topic
  Topic: msgpack-topic    TopicId: 62OJ_kUETVie1r-es9D5Mg PartitionCount: 1       ReplicationFactor: 1    Configs: segment.bytes=1073741824
          Topic: msgpack-topic    Partition: 0    Leader: 3       Replicas: 3     Isr: 3
  ```

- **为什么需要分区，也就是说主题只有一个分区，难道不行吗？日志为什么需要分段**

- **数据保存在内存还是写硬盘，如何保证数据不丢失**

  保存在内存；写日志，增加 flash 频率

- **为什么要用 zookeeper 啊，而不是存储在本地文件或者本地 DB**

  因为现在是集群啊，需要处理分布式存储的问题，数据是存储在多台服务器上的

- **消息队列的最主要作用就是保证数据正确无误传输给到对端进行处理**

- **生产者消费者模式不是很简单吗？为什么还要弄这么重的 kafka**

- **引入消息队列之前，生产者直接把消息发给消费者，中间即使出错了，可以重复几次就好了；现在多引入了一个消息队列，系统复杂度就变高了，怎么确保消息正确传输呢？**

  TCP 中保证可花了不少功夫，面向连接 + 序列号机制 + 确认机制 + 超时重传 + 校验和 + 流量控制 + 拥塞控制；现在 kafka 怎么保证消息不会丢失呢

- **还有就是消息消费的事务性如何保证，消息是放到消息队列了，当按照原架构，这条消息应该只被消费一次，不应该被多次消费的**

  生产者会不会重复提交？消费者会不会重复消费

- **MySQL、Redis 就已经够强了，kafka 是怎么做到不借助数据库的情况下，存储和读写都那么快的呢**

  - 提高读速度

    一般是利用**索引**，来提高查询速度，但是有了索引，大量写操作都会维护索引，那么会降低写入效率。常见的如关系型数据库：MySQL 等

  - 提高写速度

    一般是采用日志存储, 通过**顺序追加写**的方式来提高写入速度，因为没有索引，无法快速查询，最严重的只能一行行遍历读取。常见的如大数据相关领域的基本都基于此方式来实现。

- **kafka 内部是怎么确保 partition 有序的？**

- **`kafka_2.13-3.2.0` 怎么有两个版本号？**

  2.13 是指编译 kafka 源代码的 scala 编译器版本

  3.2.0 是 kafka 的版本，3 是大版本号 2 是小版本号 0 是修订版本号

- **为什么后序的版本不把数据存放到 zookeeper 了**

  如果想维护一个 kafka 集群，如何搭建，如何能让他们集群在一起，如果消费挂了，消费了 mq 上面的消息第 n 条，重新启动消费下一条，这个记录需要怎么处理；

  使用 zk 注册消息把这些机器，放在一起，相互注册，搭建成一个集群。关于消费到那一条，由 zk 存储一下 offset，帮助 kafka 知道消费者消费到第几条；

  这个 kafka 后面高版本放在 kafka 维护，后面会聊一下为什么要放到 kafka，举个例子，每次挂掉重新找，或者消费第几条，每次都要去 zk 去找记录，项目一般不喜欢耦合，所以后面会放到 kafka，放到主系统。

# 排障指南

- [重新安装过 kafka，导致起不来，报错 AccessDeniedException，原因是 zookeeper 数据没有清理掉](https://blog.csdn.net/dsjtlmy/article/details/88557324)

  ```sh
  # zookeeper 数据默认存储在 /tmp/zookeeper 下
  ```

- `ERROR Shutdown broker because all log dirs in /data/kafka-logs have failed (kafka.log.LogManager)`

  看日志是说 kafka 的数据错乱了；删掉所有 kafka 数据后，就可以正常了；不过 topic 也会被删掉，下次关闭重启后，还是有这个问题

- `Error while executing topic command : Replication factor: 2 larger than available brokers: 1.`

- [CentOS 没有 jps 命令](https://blog.csdn.net/ADCadc123456789/article/details/107429692)

- `kafka.common.InconsistentClusterIdException: The Cluster ID DEAdnBfwT2Cj6SNun5Vi4A doesn't match stored clusterId Some(_J80o2MhSvecnxBCUkjChA) in meta.properties. The broker is trying to join the wrong cluster. Configured zookeeper.connect may be wrong.` 各节点的集群 ID 不匹配

  [删掉 kafka 日志路径下的 meta.properties](https://www.jianshu.com/p/d51ef3369b37)，`systemctl restart zookeeper` 重启 zookeeper

- `kafka.common.InconsistentBrokerIdException: Configured broker.id 3 doesn't match stored broker.id Some(2) in meta.properties. If you moved your data, make sure your configured broker.id matches. If you intend to create a new broker, you should remove all data in your data directories (log.dirs).`

  > 克隆了一台机器，作为 node-3 加入到已有集群（node-1, node-2）

  和上面一样，删掉 meta.properties，`systemctl restart zookeeper` 重启 zookeeper

- `Received INCONSISTENT_TOPIC_ID from the leader for partition test-2. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist. (kafka.server.ReplicaFetcherThread)`

# 其他

- [`__consumer_offsets`](https://blog.csdn.net/qq_33446500/article/details/105890655) 是 kafka 自行创建的，和普通的 topic 相同。它存在的目的之一就是保存 consumer 提交的位移

  ![alt](https://img-blog.csdnimg.cn/20200518225044557.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMzNDQ2NTAw,size_16,color_FFFFFF,t_70)

- `观察者模式 + 生产者消费者模式`

  很多人把观察者模式和订阅模式混淆一谈，其实订阅模式有一个调度中心，对订阅事件进行统一管理。而观察者模式可以随意注册事件，调用事件，虽然实现原理都雷同，设计模式上有一定的差别，实际代码运用中差别在于：订阅模式中，可以抽离出调度中心单独成一个文件，可以对一系列的订阅事件进行统一管理。这样和观察者模式中的事件漫天飞就有千差万别了，在开发大型项目的时候，订阅/发布模式会让业务更清晰！

- 传统 MQ 只负责转发和被消费前临时存储，而 Kafka 本身就是数据，因此 Kafka 像数据库那样认真的考虑了数据高可用的问题，实现了多副本，自动切主等功能

- ★ 数据丢失与重复

  业务层，消费者执行判断这个数据是不是已经被消费过

- [docker 部署 kafka 集群(单台服务器)](https://www.csdn.net/tags/OtDaQg5sNzkxMDAtYmxvZwO0O0OO0O0O.html)

- kafka 数据丢失与重复

- confluent.inc --> kafka
