- [参考资料](#参考资料)
- [主从复制](#主从复制)
  - [`sync` 和 `psync` 命令的区别](#sync-和-psync-命令的区别)
    - [`sync` 命令](#sync-命令)
    - [`psync` 命令](#psync-命令)
  - [全量复制过程](#全量复制过程)
    - [全量复制的性能开销分析](#全量复制的性能开销分析)
  - [增量复制过程](#增量复制过程)
- [主从集群的缺陷](#主从集群的缺陷)
  - [主从时延问题](#主从时延问题)
  - [读取到过期数据](#读取到过期数据)
  - [规避全量复制](#规避全量复制)
  - [规避复制风暴](#规避复制风暴)
- [总结](#总结)

# 参考资料

[Redis（三）、Redis 主从复制](https://www.cnblogs.com/haoprogrammer/p/11077121.html)

[Redis 高可用篇：你管这叫主从架构数据同步原理？](https://mp.weixin.qq.com/s?__biz=MzkzMDI1NjcyOQ==&mid=2247487769&idx=1&sn=3c975ea118d4e59f72df5beed58f4768&source=41#wechat_redirect)

[docker-compose/redis_master_slave/README.md](https://github.com/M-Kepler/docker-compose/blob/master/redis_master_slave/README.md#%E4%B8%BB%E4%BB%8E%E5%90%8C%E6%AD%A5%E8%BF%87%E7%A8%8B)

# 主从复制

主从复制：主节点负责写数据，从节点负责读数据，从而实现读写分离，提高 Redis 的高可用性。

可以一主多从，或者级联部署

![alt](https://img-blog.csdnimg.cn/20200620203535535.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1Nla3lfZmVp,size_16,color_FFFFFF,t_70)

**主从复制的特点：**

- 一个 master 可以有多个 slave

- 一个 slave 只能有一个 master

- 数据流向是单向的，master 到 slave

**主从复制的作用：**

- 数据副本：多一份或多份数据拷贝，保证 Redis 高可用

- 扩展性能：单机 Redis 的性能是有限的，主从复制能横向扩展 如容量、QPS 等

## `sync` 和 `psync` 命令的区别

psync: Partial Resynchronization 部分重同步

[Redis 旧版本的 sync 和 psync](https://juejin.cn/post/6871980170460364807)

### `sync` 命令

SYNC 命令进行主从同步主要有一下几个问题：

- 主节点执行 `bgsave` 命令生成 `RDB` 文件，这个生成过程会大量消耗主节点资源（CPU、内存和磁盘 I/O 资源）。

- 主节点需要将生成的 `RBD文件发送给从节点`，这个发送操作会消耗主从节点大量的网络资源（带宽与流量）。

- 接收到 RDB 文件后，`从节点需要载入 RDB 文件`，载入期间从节点会因为阻塞而导致没办法处理命令请求(主节点不会阻塞)。

- **最大的问题是重连接后回全量同步数据**。如上面例子，slave 在断开后，再进行重新连时，slave 丢掉以前的数据，全量同步 master 数据，要是断开到重连期间执行的写命令很少，这种操作就没有必要了。

### `psync` 命令

Redis 在 2.8 及更高的版本里引入了 `psync` 命令，**实现了全量同步 和 部分同步模式**

- 全量重同步

  跟旧版复制基本是一致的，可以理解为 “全量” 复制。

- 部分重同步

  salve 断开又重新连时，在命令传播阶段，只需要发送与 master 断开这段时间执行的写命给 slave 即可，可以理解为 “增量” 复制

**`psync` 引入三个新的概念：**

- 看下节点状态

  **主节点状态**

  ```sh
  $redis-cli -p 6379 info replication
  # Replication
  role:master

  # 从节点信息
  connected_slaves:2
  slave0:ip=172.125.0.3,port=6379,state=online,offset=147914,lag=1
  slave1:ip=172.125.0.2,port=6379,state=online,offset=147914,lag=1

  # 故障转移状态
  ## no-failover 当前没有正在协商的故障转移
  ## waiting-for-sync 主节点正在等待副本来获取他的副本数据偏移值
  ## failover-in-progress 主节点已经降级，并试图将所有权移交给目标副本
  master_failover_state:no-failover

  # Redis 4.0 的 psync2 引入的两个 ID
  # master_replid2 和 second_repl_offset 都记录了当前和上一次的数据
  # master_replid 是主节点启动时生成的 40 位随机码，用来标识主节点的
  master_replid:527c2a0d6350111edd3765108337fe001ea6a03f
  master_replid2:345628ffea91df3a59ef4026e556b26962f5179c

  # 主节点的偏移量
  master_repl_offset:147914
  second_repl_offset:5175

  # 主从同步期间存放数据的缓存
  repl_backlog_active:1
  # 缓冲区容量
  repl_backlog_size:1048576
  repl_backlog_first_byte_offset:5175
  ## 环形缓冲复制队列已用大小
  repl_backlog_histlen:142740

  ```

  **从节点状态**

  ```sh
  # 相对主节点，从节点多了以下配置
  master_host:master
  master_port:6379
  master_link_status:up
  master_last_io_seconds_ago:9
  master_sync_in_progress:0
  slave_read_repl_offset:149230

  # 从节点的偏移量
  slave_repl_offset:149230

  slave_priority:100
  slave_read_only:1
  replica_announced:1
  connected_slaves:0
  ```

- **`run_id` 运行时 ID**

  Redis 每次启动时，都会生成一个不同的 id 来标示当前运行的 Redis；从节点中会保存主节点的 run_id 标示；

  当主从复制在初次复制时，主节点将自己的 runid 发送给从节点，从节点将这个 runid 保存起来, 当断线重连时，从节点会将这个 runid 发送给主节点。主节点根据 runid 判断能否进行部分复制

  如果相同，证明这个从节点和主节点是连接过的，然后根据偏移量判断是否需要全量复制

  ```sh
  $redis-cli -p 6379 info server

  # Server
  redis_version:4.0.14
  ...
  run_id:49dbc223587cbdadd158adc21816979722b65ae1
  ...
  ```

- **`slave_repl_offset` 复制偏移量**

  每当主节点增删改一个数据时，主节点中就会有一个数值来记录这种变化，`偏移量就是记录 Redis 中数据改变的一个标示`。

  当主节点更改一个数据时，偏移量也会发生对应的改变，而且主节点在将数据更改命令同步给从节点时，也会将该偏移量发送给从节点，这样就 `可以对比主从节点的偏移量，来观察是否出现主从不一致的问题`。

- **`repl_backlog_buffer` 复制积压缓冲区**

  **注意：** repl_backlog_buffer 并不是 aof，aof 是文件。。。

  复制积压缓冲区是由主节点维护的一个 `固定长度（fixed-size）先进先出（FIFO）环形队列`（每次都只保留最新的 N 条命令），默认大小为 1MB。

  ```sh
  # 环形缓冲区大小配置
  $redis-cli config get repl-backlog-size
  1) "repl-backlog-size"
  2) "1048576"  # 1MB 大小
  ```

  - 主节点除了把命令同步给从节点外，还 `会保留一份到复制积压缓存区`；并且复制积压缓冲区会为队列中的每个字节记录相应的复制偏移量

  - 从节点连接上来的时候，会把它的 `slave_repl_offset` 发过来，主节点就知道两者之间数据是否同步了，如果发现从节点发过来的偏移量都不存在，那就要做一次全量同步了（主节点偏移量：`master_repl_offset`）

## 全量复制过程

![alt](https://img-blog.csdnimg.cn/46487dd875db448ebda8ae7ed622756a.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA6ZKi54mZRw==,size_20,color_FFFFFF,t_70,g_se,x_16)

run_id 和 offset 以及 backlog 三个概念是增量复制添加进来的；原来的全量复制很挫，没有 offset 来判断是否是增量同步，每次都发送全量 RDB 快照和期间产生的数据缓存

**1. 携带 run_id 和 offset 向主节点发送 `psync` 命令请求增量同步**

- `psync offset run_id` 第一次发送不知道主节点的 `run_id` 和 `offset`，默认发送 `?:-1`

**2. 主节点收到消息**

- 根据 1 能判断出来是第一次复制，主节点把 `run_id` 和 `offset` 发送给从节点

- 判断从节点上报的 `run_id 和 主节点的是否不一致`，主节点会拒绝增量复制，需要进行全量复制

**3. 从节点保存主节点基本信息**

**4-5-6. 主节点 `fork` 出一个子进程，执行 `bgsave` 生成 RDB 快照，并把快照发送给从节点**

在此期间，会记录后续执行的数据更改命令所更改的数据到缓存中，`直到主节点将生成的 RDB 文件传输到从节点为止`

- **数据更改命令环形缓存区 `repl_backlog_buffer`**

  ![alt](https://img-blog.csdnimg.cn/8fd33fabc5324c338620abd0991150fd.png)

  当 Redis 通过 `fork()` 函数开辟一个子进程处理其他事务（比如主进程执行 `bgsave` 生成一个 RDB 文件时，或者主进程执行 `bgrewriteaof` 生成一个 AOF 文件）时；或者从节点掉线，又或者 RDB 网络传输过程等等，反正就是 **把传输数据到从节点期间产生的变更，都保存到缓存中**

  缓存是所有从库共享的。主库和从库会各自记录自己的复制进度，所以，不同的从库在进行恢复时，会把自己的复制进度（slave_repl_offset）发给主库，主库就可以和它独立同步

  主进程（即处理客户端命令的进程）后续执行的一些数据更改命令会被暂时保存在该区域，而且该区域空间有限（配置文件中 `repl_backlog_size 1mb` 即可配置该处空间大小）

  这其实是个环形的数组；所以当缓存超过 `repl_backlog_size` 时，从节点在主库 `repl_backlog_buffer` 的 `slave_repl_offset` 位置上的数据已经被覆盖掉了（原本是把 offset 与当前位置之间的数据发送给从节点就醒，但是现在被覆盖了），那么就会触发全量复制

**7-8 从节点`清空此前的所有数据，加载 RDB 文件`恢复数据并存入新更改的数据**

- 同步期间，还是可以用旧的数据对外提供服务；

- 从节点【清空自身旧数据、载入新 RDB 文件的过程是阻塞的】，无法响应客户端的命令；如果从节点执行 bgrewriteaof 也会带来额外的消耗

### 全量复制的性能开销分析

- `bgsave` 生成 RDB 文件需要的时间

- RDB 文件在 `网络传输` 时间

- 从节点的数据清空时间

- 加载 RDB 文件的时间

- 可能的 AOF 重写时间

  如果 slave node 开启了 AOF，那么会执行 BGREWRITEAOF，重写 AOF

## 增量复制过程

![alt](https://img-blog.csdnimg.cn/172828f01a74463c83d015538a203c50.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA6ZKi54mZRw==,size_20,color_FFFFFF,t_70,g_se,x_16)

全量复制会带来一个性能开销的问题，而且从节点中可能有大量数据是主节点中没有更该过的，也就是不需要进行再次同步的数据，如果使用全量复制肯定是带来了一些不必要的浪费。所以，部分复制功能就是为了解决该问题的。

- 主从节点直接连接断开，**此时主节点继续执行的数据更改命令会被记录在一个缓冲区 `repl_back_buffer` 中**

- 当从节点重新连接主节点时，自动发出一条命令（`psync offset run_id`），将从节点中存储的主节点的 Redis 运行时 `run_id` 和从节点中保存的`偏移量`发送给主节点

- 主节点接收从节点发送的 `run_id 和 偏移量`，对比此时主节点的偏移量和接收的偏移量

  `如果两个偏移量之差大于 repl_back_buffer` 中的数据，那么就表示在`断开连接期间从节点已经丢失了超出规定数量的数据`，此时就需要进行全量复制了，否则就进行部分复制

- 将主节点缓冲区中的数据同步更新到从节点中，这样就实现了部分数据的复制同步，降低了性能开销

# 主从集群的缺陷

## 主从时延问题

- 即客户端发来的读写命令分开，写命令交给主节点执行，读命令交给从节点执行，不仅减少了主节点的压力，而且增强了读操作的能力；

- 但是主从节点之间数据复制造成的阻塞延迟也可能会导致主从不一致的情况，也就是主节点先进行了写操作，但可能因为数据复制造成的阻塞延迟，导致在从节点上进行的读操作获取的数据与主节点不一致

- `主从配置不一致造成的问题有`

  比如配置中的 maxmemory 参数如果配置不一致，比如主节点 2Gb，从节点 1Gb，那么就可能会导致数据丢失；以及一些其他配置问题

## 读取到过期数据

- 主从复制会将带有过期时间的数据一并复制到从节点中，`但是从节点是没有删除数据的能力的，即使是过期数据`；

- 所以主节点中的已经删除了过期数据，但是`因为主从复制的阻塞延迟问题导致从节点中的过期数据没有删除`，此时客户端就会读到一个过期数据

## 规避全量复制

全量复制的性能开销较大，所以要尽量避免全量复制

- 在第一次建立主从节点关系式一定会发生全量复制；可以适当减小 Redis 的 maxmemory 参数，这样可以使得 RDB 更快，或者选择在客户端操作低峰期进行，比如深夜

- 从节点中保存的主节点 run_id 不一致时也一定会发生全量复制（比如主节点的重启）；可以通过故障转移来尽量避免，例如 Redis Sentinel 与 Redis Cluster

- 当主从节点的偏移量之差大于命令缓冲区 `repl_back_buffer` 中对应数据的偏移差时，也会发生全量复制，也就是上面的部分复制的复制过程中所说的；可以适当增大配置文件中 `repl_backlog_size` 即数据缓冲区可尽量避免

## 规避复制风暴

**从节点过多，导致的复制风暴**

- `当主节点重启后，要向其所有的从节点都进行一次全量复制。`

- 可以更换主从节点的拓扑结构，更换为类似树形的结构，一个主节点只与少量的从节点建立主从关系，而这些主节点又与其他从节点构成主从关系

- 限制一个 master 上的 slave 节点数量，如果实在是太多 slave； 可以采用 `主 - 从 - 从` 级联结构，减少 master 压力

  ![alt](https://img-blog.csdnimg.cn/56e475c04ff648e1bd13c46498ece7fb.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA6ZKi54mZRw==,size_20,color_FFFFFF,t_70,g_se,x_16)

**主节点部署在同一台机器，引发的复制风暴**

- 如果`一台机器专门用来部署多个主节点`，然后其他机器部署从节点，那么一旦主节点机器宕机重启，就会引起所有的主从节点之间的全量复制，造成非常大的性能开销

- 可以采用多台机器，**分散部署主节点**，或者使用自动故障转移来将某个从节点变为主节点实现一个高可用

# 总结

- 从节点连接主节点，携带 run_id 和 offset （首次连接为分别为 ? 和 -1，如果之前连接过其他的主节点，则不是默认值） 请求一次 psync 部分同步

- 主节点比对 run_id 和自身的 run_id 是否一致，发现不一致，返回要求进行增量同步；如果 run_id 一致，则返回 continue，允许部分同步

  要求？进行增量同步

- 主节点 fork 出一个子进程执行 bgsave 生成 RDB 文件，并发送给从节点

  - 这个 rdb 文件每个从节点都不一样吗？那如果收到多个从节点的并发请求，岂不是要执行多次

    如果是并发的话，只会执行一次生成 RDB，然后发给这些并发过来的从节点

  - RDB 文件如果很大的话，生成和发送过程都会很久，也有可能会失败，这期间从节点怎么办？

    从节点等待复制超时时间 repl_timeout 60s

- 从节点清空自身的旧数据，加载父节点发送过来的数据

  - 清空自身数据？那这期间怎么对外提供服务？

    是的，从节点会先清空自身旧的数据、

  - 那这期间怎么对外提供服务？

    - 主节点采用的是 bgsave，会 fork 出一个子进程来处理，所以还是可以响应请求

    - 从节点在载入新数据之前，还会用旧数据提供服务；载入新 RDB 文件时会删除旧的数据，加载新的数据，这个过程是阻塞的

- 主节点在收到从节点返回期间的所有请求，都写入到 buflog 中

  - 这个 buflog 是 AOF 吗？

    [不是，AOF 是回写磁盘，性能差多了；而且 AOF 是记录所有的操作记录，数据很大的](https://my.oschina.net/u/3847203/blog/4562322)

  - 为什么要设置一个 buflog

    `bgsave` 生成 RDB 文件需要的时间、RDB 文件在 `网络传输` 时间、从节点的数据清空时间、加载 RDB 文件的时间、可能的 AOF 重写时间

    这些操作期间产生的数据都要存储起来

- 从节点处理完 RDB 后，把主节点的 offset 与从节点的 offset 之间的缓存发送给从节点

  - 那么问题来了，发送和处理期间产生的数据怎么办呢？还是这样不断反复吗？

- 但是如果从节点太久不连接主节点，offset 已经超过 buflog 大小了，那就强制进行一次全量同步
