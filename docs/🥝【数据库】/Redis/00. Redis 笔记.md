- [参考资料](#参考资料)
- [Redis](#redis)
  - [安装](#安装)
  - [配置](#配置)
    - [配置说明](#配置说明)
    - [cluster 集群](#cluster-集群)
  - [命令](#命令)
  - [数据库](#数据库)
  - [数据结构](#数据结构)
    - [`string`](#string)
    - [`list`](#list)
    - [`hash`](#hash)
    - [`set`](#set)
    - [`zset`](#zset)
- [TTL](#ttl)
  - [设置 Redis 键过期时间](#设置-redis-键过期时间)
- [实际应用](#实际应用)
  - [发布订阅](#发布订阅)
  - [缓存系统](#缓存系统)
  - [利用 `redis` 做分页](#利用-redis-做分页)
  - [做排行榜，比如存储最近 7 天的数据](#做排行榜比如存储最近-7-天的数据)
  - [使用 redis 生成分布式循环自增的 id](#使用-redis-生成分布式循环自增的-id)
- [学习过程中的疑问](#学习过程中的疑问)
- [排障指南](#排障指南)
- [其他](#其他)

# 参考资料

- [菜鸟教程](https://www.runoob.com/redis/redis-tutorial.html)

- [Redis 设计与实现](http://redisbook.com/index.html)

- [Redis 命令参考](http://doc.redisfans.com/index.html)

# Redis

Redis 全称是 Remote DIctionary Service，即远程字典服务。

![alt](https://baiyp.ren/images/redis/redis01.jpg)

## 安装

[CentOS 安装 Redis](https://segmentfault.com/a/1190000023178516)

这么强大的 Redis，源码包 `redis-6.2.6.tar.gz` 只有 2.36M，装完也只有 5.75M。。。

## 配置

[★ Redis 安装部署（集群部署）](https://baiyp.ren/Redis%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2.html)

多了解配置，功能都是和配置强相关的，通过配置可以回顾下功能点

```sh
127.0.0.1:6379> config get maxmemory-policy
maxmemory-policy
noeviction

127.0.0.1:6379> config get daemonize
daemonize
yes
```

### 配置说明

[Redis 的启动与配置参数大全](https://www.modb.pro/db/72308)

没有默认配置文件，从官网获取 `wget http://download.redis.io/redis-stable/redis.conf`

这些配置基本都可以作为命令行启动参数，比如

```sh
redis-server --slaveof master-node 6379 --requirepass redis_pwd --masterauth redis_pwd
或：
redis-server --cluster-enabled yes --cluster-config-file nodes.conf --cluster-node-timeout 5000

这些命令行启动参数都和配置一一对应的，比如配置是 cluster-enabled yes，那么命令行启动参数就是 --cluster-enabled yes
```

```sh
# 设置后台启动，一般设置yes
daemonize yes
# edis以守护进程方式运行时,redis默认会把pid写入/var/run/redis.pid文件
pidfile /var/run/redis.pid
# 默认端口为6379
port 6379
# 主机地址，设置0.0.0.0表示都可以访问。127.0.0.1表示只允许本机访问
bind 127.0.0.1
# 客户端闲置多长时间后关闭连接，如果指定为0，表示关闭该功能
timeout 900
# 日志记录方式，默认为标准输出
logfile stdout
# 指明日志文件名
logfile "./redis7001.log"
# 设置数据库的数量，默认数据库为0
databases 16
# 有多少次更新操作，就将数据同步到数据文件

# Redis默认配置文件中提供了三个条件：
# 900秒（15分钟）内有1个更改
save 900 1
# 300秒（5分钟）内有10个更改
save 300 10
# 60秒内有10000个更改
save 60 10000

# 指定存储至本地数据库时是否压缩数据
rdbcompression yes
# 指定本地数据库文件名
dbfilename dump.rdb
# 指定本地数据库存放目录
dir ./
# 主从同步设置，设置主数据库的ip和端口
slaveof
# 如果非零，则设置SO_KEEPALIVE选项来向空闲连接的客户端发送ACK
tcp-keepalive 60
# 默认如果开启RDB快照(至少一条save指令)并且最新的后台保存失败，Redis将会停止接受写操作
# 这将使用户知道数据没有正确的持久化到硬盘，否则可能没人注意到并且造成一些灾难
stop-writes-on-bgsave-error yes
# 默认如果开启RDB快照(至少一条save指令)并且最新的后台保存失败，Redis将会停止接受写操作。
stop-writes-on-bgsave-error yes
# 当导出到 .rdb 数据库时是否用LZF压缩字符串对象
rdbcompression yes
# 版本5的RDB有一个CRC64算法的校验和放在了文件的最后。这将使文件格式更加可靠。
rdbchecksum yes
# 持久化数据库的文件名
dbfilename dump-master.rdb
# 工作目录
dir /usr/local/redis-4.0.8/redis_master/
# slav服务连接master的密码
masterauth testmaster123
# 当一个slave失去和master的连接，或者同步正在进行中，slave的行为可以有两种：
#1) 如果 slave-serve-stale-data 设置为 "yes" (默认值)，slave会继续响应客户端请求，可能是正常数据，或者是过时了的数据，也可能是还没获得值的空数据。
# 2) 如果 slave-serve-stale-data 设置为 "no"，slave会回复"正在从master同步
# （SYNC with master in progress）"来处理各种请求，除了 INFO 和 SLAVEOF 命令。
slave-serve-stale-data yes
# 配置是否仅读
slave-read-only yes
# 如果你选择“yes”Redis将使用更少的TCP包和带宽来向slaves发送数据。但是这将使数据传输到slave上有延迟，Linux内核的默认配置会达到40毫秒
# 如果你选择了 "no" 数据传输到salve的延迟将会减少但要使用更多的带宽
repl-disable-tcp-nodelay no
# slave的优先级，优先级数字小的salve会优先考虑提升为master
slave-priority 100
# 密码验证
requirepass testmaster123
# redis实例最大占用内存，一旦内存使用达到上限，Redis会根据选定的回收策略（参见：
# maxmemmory-policy）删除key
maxmemory 3gb
# 最大内存策略：如果达到内存限制了，Redis如何选择删除key。
# volatile-lru -> 根据LRU算法删除带有过期时间的key。
# allkeys-lru -> 根据LRU算法删除任何key。
# volatile-random -> 根据过期设置来随机删除key, 具备过期时间的key。
# allkeys->random -> 无差别随机删, 任何一个key。
# volatile-ttl -> 根据最近过期时间来删除（辅以TTL）, 这是对于有过期时间的key
# noeviction -> 谁也不删，直接在写操作时返回错误。
maxmemory-policy volatile-lru
# AOF开启
appendonly no
# aof文件名
appendfilename "appendonly.aof"
# fsync() 系统调用告诉操作系统把数据写到磁盘上，而不是等更多的数据进入输出缓冲区。
# 有些操作系统会真的把数据马上刷到磁盘上；有些则会尽快去尝试这么做。
# Redis支持三种不同的模式：
# no：不要立刻刷，只有在操作系统需要刷的时候再刷。比较快。
# always：每次写操作都立刻写入到aof文件。慢，但是最安全。
# everysec：每秒写一次。折中方案。
appendfsync everysec
# 如果AOF的同步策略设置成 "always" 或者 "everysec"，并且后台的存储进程（后台存储或写入AOF
# 日志）会产生很多磁盘I/O开销。某些Linux的配置下会使Redis因为 fsync()系统调用而阻塞很久。
# 注意，目前对这个情况还没有完美修正，甚至不同线程的 fsync() 会阻塞我们同步的write(2)调用。
# 为了缓解这个问题，可以用下面这个选项。它可以在 BGSAVE 或 BGREWRITEAOF 处理时阻止主进程进行fsync()。
# 这就意味着如果有子进程在进行保存操作，那么Redis就处于"不可同步"的状态。
# 这实际上是说，在最差的情况下可能会丢掉30秒钟的日志数据。（默认Linux设定）
# 如果你有延时问题把这个设置成"yes"，否则就保持"no"，这是保存持久数据的最安全的方式。
no-appendfsync-on-rewrite yes
# 自动重写AOF文件
auto-aof-rewrite-percentage 100
auto-aof-rewrite-min-size 64mb
# AOF文件可能在尾部是不完整的（这跟system关闭有问题，尤其是mount ext4文件系统时
# 没有加上data=ordered选项。只会发生在os死时，redis自己死不会不完整）。
# 那redis重启时load进内存的时候就有问题了。
# 发生的时候，可以选择redis启动报错，并且通知用户和写日志，或者load尽量多正常的数据。
# 如果aof-load-truncated是yes，会自动发布一个log给客户端然后load（默认）。
# 如果是no，用户必须手动redis-check-aof修复AOF文件才可以。
# 注意，如果在读取的过程中，发现这个aof是损坏的，服务器也是会退出的，
# 这个选项仅仅用于当服务器尝试读取更多的数据但又找不到相应的数据时。
aof-load-truncated yes
# Lua 脚本的最大执行时间，毫秒为单位
lua-time-limit 5000
# Redis慢查询日志可以记录超过指定时间的查询
slowlog-log-slower-than 10000
# 这个长度没有限制。只是要主要会消耗内存。你可以通过 SLOWLOG RESET 来回收内存。
slowlog-max-len 128
# 客户端的输出缓冲区的限制，可用于强制断开那些因为某种原因从服务器读取数据的速度不够快的客户端
client-output-buffer-limit normal 0 0 0
client-output-buffer-limit slave 256mb 64mb 60
client-output-buffer-limit pubsub 32mb 8mb 60
# 当一个子进程重写AOF文件时，文件每生成32M数据会被同步
aof-rewrite-incremental-fsync yes

```

- 查看连接客户端

  ```sh
  127.0.0.1:6379> info clients
  # Clients
  connected_clients:2
  cluster_connections:0
  maxclients:10000
  client_recent_max_input_buffer:24
  client_recent_max_output_buffer:0
  blocked_clients:0
  tracking_clients:0
  clients_in_timeout_table:0

  127.0.0.1:6379> client list

  id=437332863 addr=10.59.72.159:60339 fd=1728 name= age=11 idle=11 flags=N db=0 sub=0 psub=0 multi=-1 qbuf=0 qbuf-free=0 obl=0 oll=0 omem=0 events=r cmd=del

  输出的是每一行代表一个客户端的信息，每行包含 10 多个属性。

  - id：客户端连接的唯一标识，这个 id 是随着 Redis 的连接自增的，重启 Redis 后会重置为 0。

  - addr：客户端连接的 ip 和端口。

  - fd：socket 的文件描述符，与 lsof 命令结果中的 fd 是同一个 (可以看到连接的状态)，如果 fd=-1 代表当前客户端不是外部客户端，而是 Redis 内部的伪装客户端。

  - name：客户端的名字，后面的 client setName(客户端执行设置我这个连接的名字) 和 client getName（可以查看客户端设置的名字）两个命令会对其进行说明。
  ```

### cluster 集群

[docker-compose 创建 Redis 集群](https://baiyp.ren/Redis%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2.html#docker-compose%E6%96%B9%E5%BC%8F%E5%88%9B%E5%BB%BA%E9%9B%86%E7%BE%A4)

## 命令

- 启动

  ```sh
  # 默认端口 `6379`
  redis-server # 启动服务
  redis-cli -h 10.118.10.1 -p 6379 # 连接 -a xxx 输入密码

  # 指定配置文件启动
  redis-server /path/to/redis.conf

  # 查看版本
  redis-server -v
  ```

- 中文显示为 `unicode` 的解决方法: 连接参数加上 `--raw`

  ```sh
  $redis-cli -p 6379 -a xxxx --raw
  ```

- 查看 `json` 格式的数据

- 查看过期时间

  ```sh
  单位：秒
  ttl [key]
  ```

- 判断某个值是否存在

  ```sh
  REDIS.ismember(key, 'xxx')
  ```

- 不进入交互环境，直接在终端执行，内容输出到文件

  ```sh
  redis-cli -p 5000 hgetall device:16:status:ext > /root/a.txt
  ```

- `scan`

  通过游标分步进行的，**不会阻塞线程**

  当 SCAN 命令的`游标参数被设置为0时，服务器将开始一次新的迭代，而当redis服务器向用户返回值为 0 的游标时`，表示迭代已结束；

  所以要不断调用 `SCAN` 直到第一个返回值为 0 就表示所有结果都遍历出来了

  ```sh
  SCAN [cursor] [MATCH pattern] [COUNT count]
  # 这个 count 不是限定返回结果的数量，而是【限定服务器单次遍历的字典槽位数量(约等于)】
  ```

  - `SCAN` 命令用于迭代当前数据库中的数据库键。

  - `SSCAN` 命令用于迭代集合键中的元素。

  - `HSCAN` 命令用于迭代哈希键中的键值对。

  - `ZSCAN` 命令用于迭代有序集合中的元素（包括元素成员和元素分值）

  ```sh
  127.0.0.1:6379[1]> keys *
  1) "f"
  2) "b"
  3) "c"
  4) "e"
  5) "g"
  6) "a"
  7) "d"

  # 使用 0 作为游标，开始新的迭代
  127.0.0.1:6379[1]> scan 0
  # 第一个返回值 是用于进行下一次迭代的新游标
  # 第二个返回值 则是一个数组，这个数组中包含了所有被迭代的元素
  # 一直调用 SCAN 命令，直到命令返回游标 0 ，为一次完整遍历
  1) "0"
  2) 1) "f"
     2) "b"
     3) "c"
     4) "g"
     5) "e"
     6) "a"
     7) "d"

  # 使用的是第一次迭代时返回的游标 0 开始新的迭代
  127.0.0.1:6379[1]> scan 2
  1) "0"
  2) 1) "g"
     2) "e"
     3) "a"
     4) "d"

  127.0.0.1:6379[1]> set ee 55
  OK
  # match 正则
  127.0.0.1:6379[1]> scan 2 match e*
  1) "0"
  2) 1) "ee"
     2) "e"

  # count 指定取出多少
  127.0.0.1:6379[1]> scan 2 match e* count 1
  1) "0"
  2) 1) "ee"
  ```

- 设置过期时间

  - `EXPIRE [key] [ttl]`: 表示将键 key 的生存时间设置为 ttl 秒数

  - `PEXPIRE [key] [ttl]`: 表示将键 key 的生存时间设置为 ttl 毫秒数

  - `EXPIREAT [key] [timestamp]`: 表示将键 key 的生存时间设置为 timestamp 所指定的秒数时间戳

  - `PEXPIREAT [key] [timestamp]`: 表示将键 key 的生存时间设置为 timestamp 所指定的毫秒数时间戳

- 移除过期时间 `PERSIST [key]`

- 查看过期时间秒数 `TTL [key]`

- 查看过期时间戳 `PTTL [key]`

## 数据库

- 切换数据库

  ```sh
  127.0.0.1:6379> SELECT [index]

  # 默认使用 0 号数据库
  127.0.0.1:6379> SET db_number 0

  # 切换到 3 号数据库
  127.0.0.1:6379> SELECT 3
  127.0.0.1:6379[3]>                      # 提示符多了个 [1]

  ```

## 数据结构

```sql
-- 查看对象的内部实现，比如 embstr、ziplist、skiplist 等基础的数据结构
object encoding [key]

type [key] -- 查看对象类型
```

### `string`

- `string` 是 `Redis` 最基本的类型，一个键对应一个值。值不仅是字符串，也可以是数字

- `string` 类型是二进制安全的，意思是 Redis 的 String 类型可以包含任何数据，比如 jpg 图片或者序列化的对象。String 类型的值最大能存储 512M

### `list`

### `hash`

- [redis 返回哈希表 key 的 field 和 value](https://blog.csdn.net/Attacking_Ape/article/details/81002599)

- [redis--hash(哈希)--- 常用命令、场景](https://www.cnblogs.com/cx-code/p/13237112.html)

- [怎么保存一个列表作为哈希的值](https://blog.csdn.net/Andy_Singh/article/details/97666877)

  ```py
  # 希望保存着样的结构
  data = {
    "crc1": [1, 2, 3, 4, 5],
    "crc2": [11, 22, 33, 44, 55]
  }
  # 把列表用 json 序列化成字符串
  map(lambda x: PIPE.hmset("key_name", x[0]: json.loads(x[1])), data.items())
  PIPE.execute()
  ```

- 怎么看有多少条记录 `HLEN xxx`

- `hmset`

  ```js
  // 命令
  redis > HMSET KEY_NAME FIELD1 VALUE1 ...FIELDN VALUEN
  // 对应结构：
  KEY_NAME:[
    {
      FILED1: VALUE1
    },
    {
      FILED2: VALUE2
    }
  ]
  ```

### `set`

- [给集合的每个元素设置过期时间](https://www.cnblogs.com/chengege/p/11075023.html)

set 是 String 类型的无序集合。集合是通过 hashtable 实现的。Set 中的元素是没有顺序的，而且是没有重复的。常用命令：sdd、spop、smembers、sunion 等

### `zset`

> - sorted Set 可以通过用户额外提供一个优先级（score）的参数来为成员排序，优先级可以相同，并且是插入有序的，即自动排序
> - 和 set 相比，sorted set 关联了一个 Double 类型权重的参数 Score，使得集合中的元素能够按照 Score 进行有序排列，Redis 正是通过分数来为集合中的成员进行从小到大的排序
> - Sorted Set 的内部使用 HashMap 和跳跃表（skipList）来保证数据的存储和有序，HashMap 里放的是成员到 Score 的映射。

# TTL

## 设置 Redis 键过期时间

Redis 提供了四个命令来设置过期时间（生存时间）。

- `EXPIRE <key> <ttl>`

  表示将键 key 的生存时间设置为 ttl 秒。

- `PEXPIRE <key> <ttl>`

  表示将键 key 的生存时间设置为 ttl 毫秒。

- `EXPIREAT <key> <timestamp>`

  表示将键 key 的生存时间设置为 timestamp 所指定的秒数时间戳。

- `PEXPIREAT <key> <timestamp>`

  表示将键 key 的生存时间设置为 timestamp 所指定的毫秒数时间戳。

PS：在 Redis 内部实现中，前面三个设置过期时间的命令最后都会转换成最后一个 PEXPIREAT 命令来完成。

- 另外补充两个知识点：

  - 移除键的过期时间

    `PERSIST <key>` ：表示将 key 的过期时间移除。

  - 返回键的剩余生存时间

    `TTL <key>` ：秒 为单位返回键 key 的剩余生存时间。

    `PTTL <key>` ：毫秒 为单位返回键 key 的剩余生存时间

# 实际应用

## 发布订阅

```sh
# 向 channel:1 频道发布消息
127.0.0.1:6379> publish channel:1 huangjinjie
(integer) 1
127.0.0.1:6379>

# 订阅消息
127.0.0.1:6379> subscribe channel:1
subscribe
channel:1
1
# =====> 接收到消息时
message
channel:1
huangjinjie
```

## 缓存系统

[缓存与数据库](https://www.cnblogs.com/duanxz/p/3788366.html)

- 用过 `mysql + redis` 的模式，redis 作为数据库的缓存，查询过的数据会存放到缓存中，如果下次再来查就先检查缓冲，缓存不存在再去查数据库, 缓存中找到数据，就直接从缓存返回；每次修改数据都要持久化到数据库

## 利用 `redis` 做分页

- [基于 redis 做缓存分页 - 简书](https://www.jianshu.com/p/9c89c579b7ef)

## 做排行榜，比如存储最近 7 天的数据

- 如果数据插入是有频率的话，比如 10 分钟插入一条，那么 7 天就是 `7 * 24 * 60 // 10 = 1008` 条数据，所以如果已经知道了 7 天一共会有 1008 条数据，那么**每插入一条就利用 `ltrim` 把列表截断就行了**

  ```r
  > multi
  > lpush my_range "item1"
  > ltrim 0 1007
  > lpush my_range "item2"
  > ltrim 0 1007
  > lpush my_range "item3"
  > ltrim 0 1007
  > lpush my_range "item4"
  > ltrim 0 1007
  > lpush my_range "item5"
  > ltrim 0 1007
  ...
  > exec
  # 查看一下数据
  > lrange my_range 0 -1
  ```

- 当然也可以优化一下，每 `n` 次再 `ltrim` 一下，节省大量的 `ltrim` 操作

  ```py
  MAX_POINT = 1008
  for item in range(10000):
      # 增加个随机数，当随机数 = 1时才ltrim，也就是大概五分之一的概率
      # 减少了5倍的操作，比如本来是每lpush一次就ltrim一次，即10000次，现在变成了 10000/5 = 2000次左右
      # 也就是最多可能冗余 TODO 一点点数据
      random_int = random.randint(0, 5)
      if random_int == 1:
        REDIS.ltrim("my_range", 0, MAX_POINT - 1)
      REDIS.lpush("my_range", item)
  ```

- 以上使用 `list` 来存储，但是没有自动排序，仅仅依赖插入顺序是不行的，可以使用 **`zset`**来实现

  ```py
  # 比如现在设备上报的是一个json数据，json数据中包含上报时间，根据这个上报时间来做排序
  def _make_faker_list():
    for item in range(10000):
        # 构造在当前之前的时间
        faker_tm = time.time()) - random.randint(0, MAX_DELTA_TM * 2)
        faker.append({
            'device_id': item,
            'event_type': item,
            'occur_time': faker_tm
        })
    random.shuffle(faker)
    return faker

  def _do_cache(data):
      # 计算出7天前的时间戳
      min_tm = int(time.time() - MAX_DELTA_TM)
      for item in data:
          tm = item.get('occur_time', 0)
          value = json.dumps(item)
          REDIS.zadd(range_key, {value: tm})
          # 删除所有分数值（即这里的时间）在7天以前的数据
          print(min_tm)
          REDIS.zremrangebyscore(range_key, 0, min_tm)
  ```

## 使用 redis 生成分布式循环自增的 id

- 主要利用了 `redis` 可以原子性地执行 `lua` 脚本

  ```r
  > eval "local cur=redis.call('INCRBY', KEYS[1], 1);if(cur>9) then redis.call('set', KEYS[1], 0); cur=0; end return cur;" 1 mykey
  > get mykey
  "1"

  # 再次执行即可得到下一个自增ID

  > eval "local cur=redis.call('INCRBY', KEYS[1], 1);if(cur>9) then redis.call('set', KEYS[1], 0); cur=0; end return cur;" 1 mykey
  > get mykey
  "2"
  ```

# 学习过程中的疑问

# 排障指南

# 其他

- 查看配置中是否开启了持久化 `appendonly yes`

- [Pipeline、事务与 Lua](https://www.cnblogs.com/xuelisheng/p/10805695.html)

- [记一篇 REDIS 布隆过滤器的使用](https://zhuanlan.zhihu.com/p/89883126)

- https://blog.csdn.net/nsrainbow/article/details/49032337

- save、bgsave、bgrewriteaof

- sync、psync

- RDB 缓冲区、AOF 缓冲区 `aof_buf`（所以才要有刷盘策略）、AOF 重写缓冲区、主从同步的 `repl_backlog_buffer` 复制积压缓冲区

- flushall 清空 Redis 所有数据库；flushdb 清空当前库的数据（会清空 RDB 文件）；如果开启了 AOF 持久化，可以编辑 AOF 文件，把最后的 flush 命令去掉就可以恢复数据了
