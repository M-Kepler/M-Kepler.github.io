- [★ Redis 高可用](#-redis-高可用)
  - [`Redis` 如何保证高并发、高可用](#redis-如何保证高并发高可用)
  - [项目中是怎么部署 `Redis` 的](#项目中是怎么部署-redis-的)
  - [`Redis` 有哪些集群模式](#redis-有哪些集群模式)
- [`sentinel` 哨兵方案](#sentinel-哨兵方案)
  - [★ 说说 `Redis` 中的哨兵](#-说说-redis-中的哨兵)
  - [哨兵的原理是什么](#哨兵的原理是什么)
  - [节点通信](#节点通信)
  - [主观下线和客观下线](#主观下线和客观下线)
  - [Sentinel 领导者选举](#sentinel-领导者选举)
  - [★ 如何选举 Redis 的主节点](#-如何选举-redis-的主节点)
  - [部署技巧](#部署技巧)
  - [过程总结](#过程总结)
- [`Cluster` 集群方案](#cluster-集群方案)
  - [和哨兵模式对比](#和哨兵模式对比)
  - [哈希槽](#哈希槽)
  - [分片](#分片)
  - [节点通信](#节点通信-1)
  - [MOVED 和 ASK 重定向](#moved-和-ask-重定向)
  - [Gossip 通信协议](#gossip-通信协议)
  - [故障转移](#故障转移)
  - [集群伸缩](#集群伸缩)
  - [过程总结](#过程总结-1)
  - [问题](#问题)
    - [为什么采用哈希槽而不是一致性哈希](#为什么采用哈希槽而不是一致性哈希)
    - [为什么是设计成 16384 个槽](#为什么是设计成-16384-个槽)
- [脑裂和解决方法](#脑裂和解决方法)
- [学习过程中的疑问](#学习过程中的疑问)
- [参考资料](#参考资料)

# ★ Redis 高可用

## `Redis` 如何保证高并发、高可用

**高并发**

Redis 的单机吞吐量可以达到几万不是问题，如果想提高 Redis 的读写能力，可以用 Redis 的`主从架构`，Redis 天然支持一主多从的主备模式，单主负责写请求多从负责读请求，主从之间异步复制，把主的数据同步到从。

**高可用**

首先利用 Redis 的主从架构解决 Redis 的单点故障导致的不可用，然后`如果使用的是主从架构，那么只需要增加哨兵机制即可，就可以实现，Redis主实例宕机，自动会进行主备切换`。以此来达到 Redis 的高可用。

## 项目中是怎么部署 `Redis` 的

- 单机部署，如果要提高处理能力可以开多个实例，充分利用多核优势

  - 单机部署实现简单、维护成本低，不需要其他额外开销

  - 单点故障问题明显，一旦 Redis 挂了，所以请求将会直接打到数据库上

  - 一个 Redis 的并发能力有限，又要兼顾读写

  - 单机部署，存储也有限，数据量大，重启会变得很忙

## `Redis` 有哪些集群模式

`单机` -----> `一主多从（级联）` -----> `sentinel 集群` -----> `cluster 集群`

[Redis 集群都有哪些模式](https://www.cnblogs.com/zhuyeshen/p/11737273.html)

- 主从复制

- 哨兵模式

- Redis 官方提供的 Cluster 集群模式 (服务端)

- Jedis sharding 集群 (客户端 sharding)

- 利用中间件代理，比如豌豆荚的 codis 等

# `sentinel` 哨兵方案

[redis-Sentinel 配置](https://www.cnblogs.com/biglittleant/p/7770960.html)

[redis-sentinel 和 redis-cluster 的区别](https://blog.csdn.net/john1337/article/details/99463595)

## ★ 说说 `Redis` 中的哨兵

哨兵解决和主从不能自动故障恢复的问题，主要作用就是`监控各节点（包括主从节点和哨兵）和故障恢复`

![alt](https://static001.geekbang.org/infoq/7a/7a1e6c5ceef52c9a8c6f80e5aced9aee.png)

- **为什么需要哨兵机制**

  用来监测 `Redis` 运行状态的，如果 `Redis master` 挂了，就把 `slave` 直接顶上去，保证业务的高可用

  主从架构，主节点 master 只负责写入，写入的数据会同步的从节点。查询时去从节点拿数据。这就是主从架构。

  它的问题在于，主节点如果挂掉以后，从节点就没用了。

  Redis-sentinel 哨兵架构（哨兵也是一个 Redis 服务），除了主从节点以外，还有哨兵。这个哨兵的作用，是监测节点的健康状况，如果主节点挂掉了，哨兵会从从节点之后选举出新的主节点，如果主节点又恢复了，那它会变为从节点；一个哨兵集群可以监控多个主从；哨兵之间也可以相互监障

  作用类似于 MySQL 主从架构中的 keepalived，监控以及主从选举切换作用

- **哨兵的作用是什么**

  - `监控`

    检查主库、从库是否运行正常

  - `提醒`

    当某个节点出现问题时，会通过 API 发出通知

  - `自动故障迁移`

    当主库不能工作时，哨兵会开始一次自动故障迁移，从 slave 中选举一个出来做 master，其他的 slave 将从新的 master 去同步数据

## 哨兵的原理是什么

[你管这玩意叫哨兵?](https://mp.weixin.qq.com/s/6qhK1oHXP_VzfgR9BjYVJg)

**检查是否故障**

- `定时向节点发探测命令`，比如 `PING`，如果得不到回复，就认为是故障了；（哨兵可以从主节点获取从节点的地址等信息，这样就可以知道该探测谁了）

- 主节点虽然挂了，但是也不会停止探测的，等他重新上线就能接受到探测包

**选举新的主节点**

- 从探测命令的回复时延、复制偏移量（即，该从节点与主节点的数据同步情况，偏差多少）来判定，如果二者都一样，那就从 `uid` 来进行区分选择

**如果哨兵挂了怎么办**

- 部署多个哨兵节点，成为哨兵集群！**只要有一个哨兵节点活着就行**，这样同时都挂掉的概率就非常小了

**哪个哨兵进行主从切换**

- 已经判定主节点客观下线了，集群里那么多哨兵，由谁来进行切主备呢？方法就是从多个哨兵里推举出一个，使用 `Raft 共识推举算法` 来选出领头羊

- 这个选举的过程就是我们经常听到的：`分布式系统领域中的「共识算法」`。

## 节点通信

> **sentinel 配置文件只配置了主节点的信息，没有配置其他任何信息**，那么，哨兵之间怎么通信？哨兵怎么知道 Redis 集群信息的？

**定时任务**

每个哨兵节点维护了 3 个定时任务。定时任务的功能分别如下：

1. 通过向主节点发送 **`info`** 命令获取最新的主从结构

2. 通过 **发布订阅** 功能 （发布和订阅都是在主节点上进行，`___sentinel__:hello` 频道）获取其他哨兵节点的信息

3. 通过向哨兵集群其他节点和 Redis 集群其他节点，发送 **ping** 命令进行心跳检测，判断是否下线

**哨兵是怎么知道 Redis 集群中节点的信息**

- `哨兵的配置文件中【只需要】保存主节点的信息`，哨兵可以通过向主节点发送 `info` 命令，就可以获取到 Redis 集群所有节点的地址角色等信息

**哨兵之间是怎么相互通信**

一句话：哨兵节点只需要知道 Redis 主节点的信息就可以了，可以通过向主节点发送 `info` 命令查询其他从节点的信息；也可以通过主节点的`【发布订阅频道 __sentinel__:hello】` 来发布和获取其他哨兵节点的信息

> 哨兵集群中各个哨兵都互相连接彼此，来检查对方的可用性以及互相发送消息。但是你不需要在任何一个哨兵配置其它的哨兵的节点信息（**配置文件只配置了主节点的信息，没有配置其他 sentinel 的信息**），那哨兵之间是怎么通信的呢？

- 每个哨兵通过向 Redis 集群中的每个节点的 `__sentinel__:hello` 频道每秒发送一次消息，来宣布它的存在；

  - 哨兵集群中各个哨兵`都互相连接彼此来检查对方的可用性以及互相发送消息`。

  - 当检测到了新的 sentinel，则将其加入到自身维护的 master 监控列表中。

  ```sh
  # 哨兵定时在 __sentinel__:hello 频道发布自身的消息
  $docker-compose -f ./redis/docker-compose.yml exec master-node redis
  127.0.0.1:6379> subscribe __sentinel__:hello
  Reading messages...
  (press Ctrl-C to quit)
  1) "subscribe"
  2) "__sentinel__:hello"
  3) (integer) 1
  1) "message"
  2) "__sentinel__:hello"
  3) "172.126.0.6,26379,fdf881aa0539c0c7fbaf0d776407367a183efe58,0,mymaster,172.126.0.1,6379,0"
  1) "message"
  2) "__sentinel__:hello"
  3) "172.126.0.5,26379,a952dbf1cb84fd2ffa98577d36f3f562cecd36c5,0,mymaster,172.126.0.1,6379,0"
  1) "message"
  2) "__sentinel__:hello"
  3) "172.126.0.4,26379,faaedace7438a175774cde5cc599fb0cf56f307c,0,mymaster,172.126.0.1,6379,0"

  ```

- 集群启动后，`sentinel.cnf` 配置文件会被自动重写，主要有一下几点

  - 增加了一个 `sentinel myid` （标识哨兵节点的唯一性）

  - 自动追加哨兵节点本身的信息（这样哨兵节点之间就会相互自动发现），以及 Redis 数据服务的 slave 的信息

  - 自动移除主节点的密码

  - dir 的相对路径被修改为绝对路径

## 主观下线和客观下线

**sdown 主观下线**

- sentinel 节点主观的认为 Redis 节点下线了

- 当向 Redis 节点执行 ping 命令超时，也就是超过配置中的 `down-after-milliseconds` 没有回复，就会认为该节点挂了

**odown 客观下线**

- Redis 节点客观意义上，确确实实下线了。

## Sentinel 领导者选举

- 当主节点被判断客观下线以后，各个哨兵节点会进行投票，采用 `Raft 共识算法`，到达一定票数后，就可以确定下来哪个哨兵作为领导者了，选举出一个领导者哨兵节点，并由该领导者节点对其进行故障转移操作。

- 每个 sentinel 节点只能投票一次，每个节点都可以参与选举，都有可能成为领导者。当 Sentinel 认为主节点主观下线时，就会向其它 Sentinel 节点发送自己要成为领导者的请求；（别的节点没有同意过其它 Sentinel 节点，就会同意此请求。所以一般谁先发起谁就很大几率成为领导者）

## ★ 如何选举 Redis 的主节点

[Redis 选举原理](https://www.cnblogs.com/nijunyang/p/12508098.html)

**主节点选举原则**

- 首先过滤掉不健康的从节点；

- 然后选择优先级最高的从节点(由 slave-priority 指定)；

- 如果优先级无法区分，则选择复制偏移量 `offset` 最大的从节点（最接近主节点，数据偏差最小的）；

- 如果仍无法区分，则选择 `runid` 最小的从节点。

**故障转移**

- 更新主从状态：通过 `slaveof no one` 命令，让选出来的从节点成为主节点；

- 通过 `slaveof [ip] [port]` 命令让其他节点成为其从节点。

- 将已经下线的原主节点设置为新的主节点的从节点，当该节点重新上线后，它会成为新的主节点的从节点。（即：解释主节点挂了，哪些探测命令也不会停止，而且哨兵也会把它设置为从节点（哨兵有个配置文件））

## 部署技巧

- Sentinel 节点不应该部署在一台服务器上

  > 和不要把主节点部署在一个服务器上来避免复制风暴是一个道理

  同一台物理机意味着，如果这台机器有什么硬件故障，所有的虚拟机都会受到影响

- 部署至少三个且`奇数`个的哨兵节点

  因为偶数的话可能出现票平等的问题。

  增加 Sentinel 节点的个数提高对于故障判定的准确性，因为故障转移时领导者选举采用过半原则，奇数个节点可以最低限度满足该条件。

- 为每个业务场景部署一套 Sentinel。

## 过程总结

- 每个 Sentinel 以 每秒钟 一次的频率，向它所知的 主节点、从节点 以及其他 Sentinel 实例 发送一个 PING 命令。来确定这些节点是否可以到达。

- 如果一个 实例 距离 最后一次 有效回复 PING 命令的时间超过 `down-after-milliseconds` 所指定的值，那么这个实例会被 Sentinel 标记为 主观下线。

- 如果一个 主节点 被标记为 主观下线，那么正在 监视 这个 主节点 的所有 Sentinel 节点，要以 每秒一次 的频率确认 主节点 的确进入了 主观下线 状态。

- 如果一个 主节点 被标记为 主观下线，并且有 足够数量 的 Sentinel（至少要达到 配置文件 指定的数量）在指定的 时间范围内同意这一判断，那么这个 主节点 被标记为 客观下线。

- 在一般情况下， 每个 Sentinel 会以每 10 秒一次的频率，向它已知的所有 主节点 和 从节点 发送 `【INFO 命令】`。当一个主节点 被 Sentinel 标记为 客观下线 时，Sentinel 向下线主节点 的所有从节点发送 INFO 命令的频率，会从 10 秒一次改为每秒一次。

- Sentinel 和其他 Sentinel 协商 主节点 的状态，如果 主节点 处于 SDOWN（主观下线）状态，则投票自动选出新的主节点。将剩余的 从节点 指向 新的主节点 进行 数据复制。

- 如果有多个 Sentinel 同时竞选，并且可能存在票数一致的情况，就会 `等待下次的一个随机时间`再次发起竞选请求（怪不得之前故障转移时快时慢呢），进行新的一轮投票，直到选出来领导者。

- 当没有足够数量的 Sentinel 同意 主节点下线时， 主节点 的客观下线状态 就会被移除。当 主节点 重新向 Sentinel 的 PING 命令返回 有效回复 时，主节点 的 主观下线状态就会被移除。

# `Cluster` 集群方案

[`Redis Cluster` 切片集群模式原理](https://juejin.cn/post/6844903984294002701)

[认识 Redis 集群——Redis Cluster](https://www.cnblogs.com/jian0110/p/14002555.html)

[Redis 高可用篇：Cluster 集群能支撑的数据有多大？](https://mp.weixin.qq.com/s?__biz=MzkzMDI1NjcyOQ==&mid=2247487789&idx=1&sn=7f8245f8b4e4a98aa0a717011f7b7e24&scene=21#wechat_redirect)

## 和哨兵模式对比

- `哨兵模式`存在难以扩容以及【单机存储】、读写能力受限的问题，并且集群之前都是一台 `Redis` 都是全量的数据，这样所有的 `Redis` 都冗余一份，就会大大消耗内存空间

- `cluster 集群模式`实现了数据的分布式存储，实现 `数据的分片`，解决单机部署容量有限的问题，每个节点存储不同的内容，并且解决了在线的节点收缩（下线）和扩容（上线）问题。

- Redis Cluster 是不保证 Redis 高可用的，保证 Redis 高可用的是主从复制加哨兵模式

**sentinel 模式存在的问题**

- 所有的写操作都集中到主节点上，主节点 CPU 压力比较大

- 不管是主节点还是从节点，它们都同样保存了 Redis 的所有数据，随着数据越来越多，可能会出现内存不够用的问题

**cluster 集群**

- Cluster 中有一个 **`16384`** 长度的槽的概念，这个槽是一个虚拟的槽，并不是真正存在的。

- 正常工作的时候，Cluster 中的 **每个 Master 节点都会负责一部分的槽**，当有某个 key 被映射到某个 Master 负责的槽，那么这个 Master 负责为这个 key 提供服务。对于 cluster 而言，不同的 key 往往 由不同的 node 处理。这样就可以把压力分摊到多台服务器上，做成分布式存储

- 如下三个 master，会把 0-16383 范围的槽可能分成三部分（0-5000）、（5001-11000）、（11001-16383）分别数据三个缓存节点的槽范围。

  ![alt](https://mmbiz.qpic.cn/mmbiz_png/IJUXwBNpKlgRctTenvgHvQOc5sGoMkmmJsMmN5z6UZaT5YBCd6vVTKugUMeoh7Lnic8yTsicicvBokoeJU7aYBUNA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

## 哈希槽

- Cluster 没有使用一致性 hash，而是引入了 **哈希槽（Hash Slot）** 的概念。来处理数据与实例之间的映射关系；Redis 集群一共有 16384 个哈希槽

- 当数据库中的 16384 个槽都分配了节点时，集群处于上线状态（ok）；如果有任意一个槽没有分配节点，则集群处于下线状态（fail）。

  **哈希槽计算方法**

  ```sh
  # 对 key 进行 CRC16 校验并对 16384 取模，计算出 key 所在的槽
  # 然后再到对应的槽上进行取数据或者存数据，这样就实现了数据的访问更新
  HASH_SLOT = CRC16(key) mod 16384
  ```

`Cluster` 集群方案就是搞多个 Redis 实例，搞成分布式，一个机器放一部分数据，所以就涉及到`【负载均衡】、寻址`的问题，客户端如何访问到对应的数据；可以分为 `服务端分片`、`客户当分片` 两种模式

## 分片

- **客户端分片模式**

  客户端与 Redis 节点直连, 不需要中间 Proxy 层，直接连接任意一个 Master 节点

  根据公式 `HASH_SLOT=CRC16(key) mod 16384`，计算出映射到哪个分片上，然后 Redis 会去相应的节点进行操作

  ![alt](https://mmbiz.qpic.cn/mmbiz_jpg/gB9Yvac5K3M9JeWwVHia7MXxHm00kiau910ormsOMWfVDQDuIA2KibVd5AvUjI0X1DXbOgWfQgMlSLtw023l6ylyQ/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

- 客户端通过哈希算法算出了哈希槽的位置，那客户端怎么知道这个哈希槽在哪台 Redis 实例上呢？

  **在集群的中每个 Redis 实例都会向其他实例「传播」自己所负责的哈希槽有哪些（Gossip 协议）。这样一来，每台 Redis 实例就可以记录着「所有哈希槽与实例」的关系了**

  有了这个映射关系以后，客户端也会「缓存」一份到自己的本地上，那自然客户端就知道去哪个 Redis 实例上操作了

- **服务端分片模式**

  就是抽象出一个代理层，客户端访问的是代理，由代理去进行路由，和分库分表的处理一样

  ![alt](https://mmbiz.qpic.cn/mmbiz_jpg/gB9Yvac5K3M9JeWwVHia7MXxHm00kiau91MUuMYyI8HRYvaLW2HMicP6w6ib4T93ZV6SbnlBd3o7swRoSZFD3ibuENQ/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

## 节点通信

节点之间实现了将数据进行分片存储，那么节点之间又是怎么通信的呢？这个和前面哨兵模式讲的命令基本一样。

- 首先新上线的节点，会通过 `Gossip` 协议向老成员发送 Meet 消息，表示自己是新加入的成员。

- 老成员收到 Meet 消息后，在没有故障的情况下会回复 PONG 消息，表示欢迎新结点的加入，除了第一次发送 Meet 消息后，之后都会发送定期 PING 消息，实现节点之间的通信。

  ![alt](https://mmbiz.qpic.cn/mmbiz_png/IJUXwBNpKlgRctTenvgHvQOc5sGoMkmm47l9D0s7d31OPcYR1hZxrxOHUtUYia1voLwlkIghILUJwNg7zUaVibcA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

- 通信的过程中会为每一个通信的节点开通一条 tcp 通道，之后就是定时任务，不断的向其它节点发送 PING 消息，这样做的目的就是为了了解节点之间的元数据存储情况，以及健康状况，以便即使发现问题。

## MOVED 和 ASK 重定向

**MOVED**

如果键所在的 slot 刚好指派给了当前节点，会直接执行这个命令。否则，节点向客户端返回 `MOVED` 错误，指引客户端转向 redirect 至正确的节点，并再次发送此前的命令。

![alt](http://upload-images.jianshu.io/upload_images/10007098-7c1a117c179541f4.png)

**ASK**

Ask 重定向发生于集群伸缩时，集群伸缩会导致槽迁移，当我们去源节点访问时，此时数据已经可能已经迁移到了目标节点，使用 Ask 重定向来解决此种情况。

例如：一个 slot 存在三个 key，分别为 hello1、hello2、hello3，假设此时 ==slot 正在处于迁移状态==，hello1 已经迁移到了目标节点，此时如果在源节点获取 hello1，则会报出 ask 重定向错误。

![alt](http://upload-images.jianshu.io/upload_images/10007098-572b0de1d100d710.png)

## Gossip 通信协议

与集中式（如使用 zookeeper 进行分布式协调注册）不同，Redis Cluster 使用的是 Gossip 协议进行通信。==并不是将集群元数据存储在某个节点上，而是不断的互相通信，保持整个集群的元数据是完整的。== Gossip 协议所有节点都持有一份元数据，不同节点的元数据发生了变更，就不断的将元数据发送给其他节点，让其他节点也进行元数据的变更。

集中式的好处：元数据的读取和更新时效性很好，一旦元数据变化就更新到集中式存储，缺点就是元数据都在一个地方，可能导致元数据的存储压力。

对于 Gossip 来说：元数据的更新会有延时，会降低元数据的压力，缺点是操作时元数据更新可能会导致集群的操作有一些滞后。

## 故障转移

[018. Redis Cluster 故障转移原理](https://cloud.tencent.com/developer/article/1605715)

- 节点之间相互通信，相互选举，不再依赖 sentinel：准确来说是**主节点之间相互 “监督”**，保证及时故障转移

- 和哨兵差不多，也有主观下线、客观下线、选举这样的说法

- 集群中主节点下线时，复制此主节点的所有的从节点将会选出一个节点作为新的主节点，并完成故障转移。和主从复制的配置相似，当原先的从节点再次上线，它会被作为新主节点的的从节点存在于集群中。

## 集群伸缩

- 节点的收缩和扩容时，会重新计算每一个节点负责的槽范围，并发根据虚拟槽算法，将对应的数据更新到对应的节点。

- 还有前面的讲的新加入的节点会首先发送 Meet 消息，详细可以查看前面讲的内容，基本一样的模式。

- 以及发生故障后，哨兵老大节点的选举，master 节点的重新选举，slave 怎样晋升为 master 节点，可以查看前面哨兵模式选举过程。

## 过程总结

## 问题

### 为什么采用哈希槽而不是一致性哈希

[Redis 集群为什么不用一致性哈希算法](https://www.jianshu.com/p/3f1c801b22ff)

**固定取模**

常见哈希算法中的取模法，就是对固定的节点输进行取模，比如现在有三个节点，那么 key 应该落在 key % 3 编号的节点上

**一致性哈希**

> 弥补固定取模哈希的伸缩性差的问题，随便增加/删减节点，都会导致大规模的哈希失效

不是说算法有多牛，算法还是哈希算法，只不过现在对节点进行哈希运算后取模了，节点落在 1-2^32 的哈希环上；数据也同样进行哈希运算后取模，也是落在哈环节上；然后顺时针找到第一个节点，就是这个数据落在的地方

**哈希槽**

`预先分配好`真实节点管理的哈希槽（slot），并存储管理起来，我们可以预先知道某个节点拥有哪些哈希槽（slot）, 这里总数是 16384

**为什么采用哈希槽而不是一致性哈希**

我们知道，一致性哈希某个节点挂了，数据会顺延存放到下一个节点；但是对于 Redis 缓存服务器来说：

- 无法根据服务器性能，手动设置数据的分布。而哈希槽可以很灵活的配置每个节点占用哈希槽的数量

- 一致性哈希有数据倾斜和节点雪崩的风险

  某节点崩了，流量会顺时针打到下一个节点，下一个节点崩溃的可能性就变高了（数据倾斜），严重的情况下，这个雪球会越滚越大，造成整个 Redis 集群的雪崩（节点雪崩）

- 但是如果采用哈希槽就可以避免这种情况，因为槽是实现算好的，请求肯定是落在某个节点上，这个节点挂了，`最多影响这些槽而已，不会导致其他节点也崩溃`

### 为什么是设计成 16384 个槽

16383 个槽数是固定的，原因可以看这篇文章：[【原创】为什么 Redis 集群有 16384 个槽](https://www.cnblogs.com/rjzheng/p/11430592.html)

理论上 CRC16 算法可以得到 2 的 16 次方个数值，其数值范围在 0-65535 之间，取模运算 key 的时候，应该是 crc16(key)%65535，但是却设计为 crc16(key)%16384

**原因是作者在设计的时候做了空间上的权衡，觉得节点最多不可能超过 1000 个，同时为了保证节点之间通信效率，所以采用了 2^14。**

# 脑裂和解决方法

[Redis 集群（主从）脑裂及解决方案](https://blog.csdn.net/LO_YUN/article/details/97131426)

**脑裂问题**

Redis 的集群脑裂是指因为网络问题，导致 Redis master 节点跟 Redis slave 节点和 sentinel 集群处于不同的网络分区

此时因为 sentinel 集群无法感知到 master 的存在，所以将 slave 节点提升为 master 节点。此时存在两个不同的 master 节点，就出现了脑裂现象

**脑裂的危害**

如果客户端还在基于原来的 master 节点继续写入数据，那么新的 master 节点将无法同步这些数据，当网络问题解决之后，sentinel 集群将原先的 master 节点降为 slave 节点，此时再从新的 master 中同步数据，`将会造成大量的数据丢失`。

**解决方法**

修改 Redis 配置，设置 `min-slaves-to-write 3` 表示要求至少有三个 slave 节点；`min-slaves-max-lag 10` 表示主从数据同步超时时间设置为 10s

配置了这两个参数之后，如果发生集群脑裂，原先的 master 节点接收到客户端的写入请求会拒绝，就可以减少数据同步之后的数据丢失。

# 学习过程中的疑问

- **cluster 的故障迁移比较模糊，只看到他是分片的。有没有主从模式的读写分离？有没有 sentinel 的故障检测选举和主从切换功能？**

  还是说 cluster 只是进行切分，把数据分到多个服务器上，然后统一对外提供服务。那是不是 cluster 后面还要部署一套 sentinel+主从

  主从节点之间并没有读写分离， Slave 只用作 Master 宕机的高可用备份。

  哨兵采用的是投票

  Redis Cluster 无需部署哨兵集群，集群内 Redis 节点通过 Gossip 协议互相探测健康状态，在故障时可发起自动切换。

- **cluster 伸缩时如何增删节点，哈希槽怎么办，数据怎么办？如果只是把某个哈希槽分摊出去，并不能起到为集群其他节点减轻压力**

  添加节点的时候，可以通过 `redis-cli --cluster reshard` 来从其他节点迁移数据到新节点，可以执行选择要从哪些节点迁出数据

# 参考资料

[一文把 Redis 主从复制、哨兵、Cluster 三种模式摸透](https://mp.weixin.qq.com/s/2weAtBfuu5Qlyvqx8h9zNw)

[docker-compose/redis_cluster/README.md](https://github.com/M-Kepler/docker-compose/blob/master/redis_cluster/README.md)
