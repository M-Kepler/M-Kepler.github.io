- [前言](#前言)
- [TCP 队头阻塞](#tcp-队头阻塞)
- [HTTP 队头阻塞](#http-队头阻塞)
  - [HTTP 管道化是什么](#http-管道化是什么)
  - [HTTP 管道化产生的背景](#http-管道化产生的背景)
  - [HTTP 管道化的限制](#http-管道化的限制)
  - [HTTP 管道化引起的请求队头阻塞](#http-管道化引起的请求队头阻塞)
- [如何解决队头阻塞](#如何解决队头阻塞)
  - [如何解决 HTTP 队头阻塞](#如何解决-http-队头阻塞)
  - [如何解决 TCP 队头阻塞](#如何解决-tcp-队头阻塞)
- [总结](#总结)

> 本文由 [简悦 SimpRead](http://ksria.com/simpread/) 转码， 原文地址 [blog.csdn.net](https://blog.csdn.net/weixin_34364071/article/details/91416530)

## 前言

通常我们提到队头阻塞，指的可能是 TCP 协议中的队头阻塞，但是 HTTP1.1 中也有一个类似 TCP 队头阻塞的问题，下面各自介绍一下。

## TCP 队头阻塞

队头阻塞（head-of-line blocking）发生在一个 TCP 分节丢失，导致其后续分节不按序到达接收端的时候。该后续分节将被接收端一直保持直到丢失的第一个分节被发送端重传并到达接收端为止。该后续分节的延迟递送确保接收应用进程能够按照发送端的发送顺序接收数据。这种为了达到完全有序而引入的延迟机制非常有用，但也有不利之处。

假设在单个 TCP 连接上发送语义独立的消息，比如说服务器可能发送 3 幅不同的图像供 Web 浏览器显示。为了营造这几幅图像在用户屏幕上并行显示的效果，服务器先发送第一幅图像的一个断片，再发送第二幅图像的一个断片，然后再发送第三幅图像的一个断片；服务器重复这个过程，直到这 3 幅图像全部成功地发送到浏览器为止。

要是第一幅图像的某个断片内容的 TCP 分节丢失了，客户端将保持已到达的不按序的所有数据，直到丢失的分节重传成功。这样不仅延缓了第一幅图像数据的递送，也延缓了第二幅和第三幅图像数据的递送。

## HTTP 队头阻塞

上面用浏览器请求图片资源举例子，但实际上 HTTP 自身也有类似 TCP 队头阻塞的情况。要介绍 HTTP 队头阻塞，就需要先讲讲 HTTP 的管道化（pipelining）。

### HTTP 管道化是什么

HTTP1.1 允许在持久连接上可选的使用请求管道。这是相对于 keep-alive 连接的又一性能优化。在相应到达之前，可以将多条请求放入队列，当第一条请求发往服务器的时候，第二第三条请求也可以开始发送了，在高延时网络条件下，这样做可以降低网络的环回时间，提高性能。

非管道化与管道化的区别示意

### HTTP 管道化产生的背景

在一般情况下，HTTP 遵守 “请求 - 响应” 的模式，也就是客户端每次发送一个请求到服务端，服务端返回响应。这种模式非常容易理解，但是效率并不是那么高，为了提高速度和效率，人们做了很多尝试：

- 最简单的情况下，服务端一旦返回响应后就会把对应的连接关闭，客户端的多个请求实际上是串行发送的。
- 除此之外，客户端可以选择同时创建多个连接，在多个连接上并行的发送不同请求。但是创建更多连接也带来了更多的消耗，当前大部分浏览器都会限制对同一个域名的连接数。
- 从 HTTP1.0 开始增加了持久连接的概念（HTTP1.0 的 Keep-Alive 和 HTTP1.1 的 persistent），可以使 HTTP 能够复用已经创建好的连接。客户端在收到服务端响应后，可以复用上次的连接发送下一个请求，而不用重新建立连接。
- 现代浏览器大多采用并行连接与持久连接共用的方式提高访问速度，对每个域名建立并行地少量持久连接。
- 而在持久连接的基础上，HTTP1.1 进一步地支持在持久连接上使用管道化（pipelining）特性。管道化允许客户端在已发送的请求收到服务端的响应之前发送下一个请求，借此来减少等待时间提高吞吐；如果多个请求能在同一个 TCP 分节发送的话，还能提高网络利用率。但是因为 HTTP 管道化本身可能会导致队头阻塞的问题，以及一些其他的原因，现代浏览器默认都关闭了管道化。

### HTTP 管道化的限制

1. 管道化要求服务端按照请求发送的顺序返回响应（FIFO），原因很简单，HTTP 请求和响应并没有序号标识，无法将乱序的响应与请求关联起来。

2. 客户端需要保持未收到响应的请求，当连接意外中断时，需要重新发送这部分请求。

3. 只有幂等的请求才能进行管道化，也就是只有 GET 和 HEAD 请求才能管道化，否则可能会出现意料之外的结果

### HTTP 管道化引起的请求队头阻塞

前面提到 HTTP 管道化要求服务端必须按照请求发送的顺序返回响应，那如果一个响应返回延迟了，那么其后续的响应都会被延迟，直到队头的响应送达。

## 如何解决队头阻塞

### 如何解决 HTTP 队头阻塞

对于 HTTP1.1 中管道化导致的请求 / 响应级别的队头阻塞，可以使用 HTTP2 解决。HTTP2 不使用管道化的方式，而是引入了帧、消息和数据流等概念，每个请求 / 响应被称为消息，每个消息都被拆分成若干个帧进行传输，每个帧都分配一个序号。每个帧在传输是属于一个数据流，而一个连接上可以存在多个流，各个帧在流和连接上独立传输，到达之后在组装成消息，这样就避免了请求 / 响应阻塞。

当然，即使使用 HTTP2，如果 HTTP2 底层使用的是 TCP 协议，仍可能出现 TCP 队头阻塞。

### 如何解决 TCP 队头阻塞

TCP 中的队头阻塞的产生是由 TCP 自身的实现机制决定的，无法避免。想要在应用程序当中避免 TCP 队头阻塞带来的影响，只有舍弃 TCP 协议。

比如 google 推出的 [quic](https://link.juejin.im?target=https%3A%2F%2Fkm.sankuai.com%2Fpage%2F108687233) 协议，在某种程度上可以说避免了 TCP 中的队头阻塞，因为它根本不使用 TCP 协议，而是在 UDP 协议的基础上实现了可靠传输。而 UDP 是面向数据报的协议，数据报之间不会有阻塞约束。

此外还有一个 SCTP（流控制传输协议），它是和 TCP、UDP 在同一层次的传输协议。SCTP 的多流特性也可以尽可能的避免队头阻塞的情况。

## 总结

从 TCP 队头阻塞和 HTTP 队头阻塞的原因我们可以看到，出现队头阻塞的原因有两个：

1. 独立的消息数据都在一个链路上传输，也就是有一个 “队列”。比如 TCP 只有一个流，多个 HTTP 请求共用一个 TCP 连接

2. 队列上传输的数据有严格的顺序约束。比如 TCP 要求数据严格按照序号顺序，HTTP 管道化要求响应严格按照请求顺序返回

所以要避免队头阻塞，就需要从以上两个方面出发，比如 quic 协议不使用 TCP 协议而是使用 UDP 协议，SCTP 协议支持一个连接上存在多个数据流等等。

转载于: https://juejin.im/post/5ce37660f265da1bb13f05f0
